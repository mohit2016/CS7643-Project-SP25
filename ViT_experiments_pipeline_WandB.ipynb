{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"LYSEhsPs0Ulr","outputId":"8356889a-f2fe-460b-f7f3-ef19ca466557","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1745212812670,"user_tz":240,"elapsed":140413,"user":{"displayName":"Jay ra1","userId":"06251653160264065784"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n","Collecting pip\n","  Downloading pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\n","Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pip\n","  Attempting uninstall: pip\n","    Found existing installation: pip 24.1.2\n","    Uninstalling pip-24.1.2:\n","      Successfully uninstalled pip-24.1.2\n","Successfully installed pip-25.0.1\n","Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers) (0.30.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.3.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.1.31)\n","Collecting datasets\n","  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n","Collecting evaluate\n","  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n","Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n","Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n","Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","Installing collected packages: xxhash, fsspec, dill, multiprocess, datasets, evaluate\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.3.2\n","    Uninstalling fsspec-2025.3.2:\n","      Successfully uninstalled fsspec-2025.3.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.12.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m117.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m196.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n","Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2024.12.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (75.2.0)\n","Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n","Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n","Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n","Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.26.1)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n","Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.1)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n","Name: wandb\n","Version: 0.19.9\n","Summary: A CLI and library for interacting with the Weights & Biases API.\n","Home-page: \n","Author: \n","Author-email: Weights & Biases <support@wandb.com>\n","License: MIT License\n","\n","Copyright (c) 2021 Weights and Biases, Inc.\n","\n","Permission is hereby granted, free of charge, to any person obtaining a copy\n","of this software and associated documentation files (the \"Software\"), to deal\n","in the Software without restriction, including without limitation the rights\n","to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n","copies of the Software, and to permit persons to whom the Software is\n","furnished to do so, subject to the following conditions:\n","\n","The above copyright notice and this permission notice shall be included in all\n","copies or substantial portions of the Software.\n","\n","THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n","IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n","FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n","AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n","LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n","OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n","SOFTWARE.\n","Location: /usr/local/lib/python3.11/dist-packages\n","Requires: click, docker-pycreds, gitpython, platformdirs, protobuf, psutil, pydantic, pyyaml, requests, sentry-sdk, setproctitle, setuptools, typing-extensions\n","Required-by: \n","Collecting schedulefree\n","  Downloading schedulefree-1.4.1-py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from schedulefree) (2.6.0+cu124)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from schedulefree) (4.13.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->schedulefree) (3.18.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->schedulefree) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->schedulefree) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->schedulefree) (2024.12.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->schedulefree) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->schedulefree) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->schedulefree) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->schedulefree) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->schedulefree) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->schedulefree) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->schedulefree) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->schedulefree) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->schedulefree) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->schedulefree) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->schedulefree) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->schedulefree) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->schedulefree) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->schedulefree) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->schedulefree) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->schedulefree) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->schedulefree) (3.0.2)\n","Downloading schedulefree-1.4.1-py3-none-any.whl (55 kB)\n","Installing collected packages: schedulefree\n","Successfully installed schedulefree-1.4.1\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (5.10.4)\n","Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat) (2.21.1)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat) (4.23.0)\n","Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.11/dist-packages (from nbformat) (5.7.2)\n","Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.11/dist-packages (from nbformat) (5.7.1)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat) (25.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat) (0.24.0)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.3.7)\n","Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat) (4.13.2)\n"]}],"source":["!pip install --upgrade pip\n","!pip install tokenizers\n","!pip install datasets --upgrade evaluate\n","!pip install transformers\n","!pip install numpy torch matplotlib pandas scikit-learn tqdm pillow\n","!pip install datasets evaluate transformers\n","!pip install torchvision\n","!pip install setuptools\n","!pip install wandb\n","!pip show wandb\n","!pip install schedulefree\n","!pip install nbformat"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"nW76RvsBd0lZ","executionInfo":{"status":"ok","timestamp":1745212853503,"user_tz":240,"elapsed":40837,"user":{"displayName":"Jay ra1","userId":"06251653160264065784"}}},"outputs":[],"source":["import os\n","import numpy as np\n","import torch\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from datasets import load_dataset\n","from evaluate import load\n","from transformers import (\n","    ViTFeatureExtractor,\n","    ViTForImageClassification,\n","    TrainingArguments,\n","    Trainer,\n","    get_scheduler,\n","    AutoImageProcessor\n",")\n","\n","from torch.optim import AdamW, SGD\n","import wandb\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","import torchvision.transforms as transforms\n","from PIL import Image\n","import random\n","from tqdm.auto import tqdm\n","from sklearn.metrics import confusion_matrix\n","from schedulefree import AdamWScheduleFree\n","from torch.optim.lr_scheduler import CyclicLR, ExponentialLR, ReduceLROnPlateau\n","from transformers import TrainerCallback, EarlyStoppingCallback\n","from scipy.ndimage import zoom\n","import os\n","import re\n","from PIL import Image"]},{"cell_type":"code","source":["class Attentionmapcallback(TrainerCallback):\n","    def __init__(self, feature_extractor, image, output_dir,total_epochs,steps_tune, device = 'cuda'):\n","        self.feature_extractor = feature_extractor\n","        self.image = image\n","        self.output_dir = output_dir\n","        self.total_epochs = total_epochs\n","        self.device = device\n","        self.steps_tune = steps_tune\n","        os.makedirs(self.output_dir, exist_ok=True)\n","\n","    def plt_attn(self, attentions, epoch, step , layer = 0, head = 0):\n","\n","      attn = attentions[layer][0,head]  #.cpu().detach().numpy()\n","      cls_attn = attn[0,1:]\n","      nm_patch = cls_attn.shape[0]\n","      gid_size = int(np.sqrt(nm_patch))\n","      ##cls_attn = cls_attn.reshape(gid_size,gid_size)\n","\n","      if gid_size * gid_size != nm_patch:\n","        print(f\"skipping attn epoch {epoch}\")\n","        return\n","\n","      cls_attn = cls_attn.cpu().detach().numpy().reshape(gid_size,gid_size)\n","\n","      if isinstance(self.image, np.ndarray):\n","        img_np = self.image\n","      else:\n","        img_np = np.array(self.image)\n","\n","      attn_resized = zoom(cls_attn, (img_np.shape[0] / gid_size, img_np.shape[1] / gid_size))\n","\n","      plt.figure(figsize=(8, 8))\n","\n","\n","      plt.imshow(self.image)\n","\n","      plt.imshow(attn_resized, cmap = 'jet',  alpha=0.3)\n","\n","      plt.axis('off')\n","      plt.title(f\"Epoch {epoch}, Layer {layer}, Head {head}\")\n","      plt.savefig(os.path.join(self.output_dir, f\"step_{step}_epoch_{epoch}_layer_{layer}_head_{head}.png\"))\n","      plt.close()\n","\n","\n","\n","    def on_step_end(self, args, state, control, **kwargs):\n","      epoch = state.epoch\n","      step = state.global_step\n","      if step in self.steps_tune:\n","        model = kwargs['model']\n","        model.eval()\n","        inputs = self.feature_extractor(images=self.image, return_tensors=\"pt\").to(self.device)\n","        with torch.no_grad():\n","          outputs = model(**inputs, output_attentions=True)\n","          attentions = outputs.attentions\n","          #self.plt_attn(attentions, int(epoch), int(step))\n","          self.plt_attn(attentions, int(epoch), int(step) ,layer = -1, head = -1)\n","\n"],"metadata":{"id":"dD1l-ioBkT1V","executionInfo":{"status":"ok","timestamp":1745212853507,"user_tz":240,"elapsed":1,"user":{"displayName":"Jay ra1","userId":"06251653160264065784"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def stp_extrct(fname):\n","  match = re.search(r'step_(\\d+)_epoch_(\\d+)', fname)\n","  return int(match.group(1)) if match else flat('inf')\n","\n","def make_log_fig(step_func,num_epoch, sch_name, opt_name):\n","  img_files = []\n","  # for i in range(num_epoch):\n","  #   for j in step_func:\n","  #     img_addr = './attention_maps/' + str(sch_name).split('.')[-1] + \"_\" + str(opt_name).split('.')[-1] + \"/\" + \"step_\" + str(j) + \"_epoch_\" + str(i) + \"_layer_-1_head_-1.png\"\n","  #     img_files.append(img_addr)\n","  img_files_names = os.listdir('./attention_maps/' + str(sch_name).split('.')[-1] + \"_\" + str(opt_name).split('.')[-1])\n","  img_files_names.sort(key=stp_extrct)\n","  for i in img_files_names:\n","    img_addr = './attention_maps/' + str(sch_name).split('.')[-1] + \"_\" + str(opt_name).split('.')[-1] + \"/\" + i\n","    img_files.append(img_addr)\n","\n","\n","  images = [Image.open(file) for file in img_files]\n","  min_height = min(img.height for img in images)\n","  images = [img.resize((img.width, min_height)) for img in images]\n","  total_width = sum(img.width for img in images)\n","  result = Image.new('RGB', (total_width, min_height))\n","  x_offset = 0\n","  for img in images:\n","      result.paste(img, (x_offset, 0))\n","      x_offset += img.width\n","\n","  output_path = './attention_maps/' + str(sch_name).split('.')[-1] + \"_\" + str(opt_name).split('.')[-1] + \"attn_images.png\"\n","\n","  result.save(output_path)\n","  logimg = \"attn_image_\" + str(sch_name).split('.')[-1] + \"_\" + str(opt_name).split('.')[-1]\n","\n","  wandb.log({logimg: wandb.Image(output_path)})"],"metadata":{"id":"Dtd8PlMKknM7","executionInfo":{"status":"ok","timestamp":1745212853509,"user_tz":240,"elapsed":1,"user":{"displayName":"Jay ra1","userId":"06251653160264065784"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"M5m2CPNN0Ulu","executionInfo":{"status":"ok","timestamp":1745212853911,"user_tz":240,"elapsed":401,"user":{"displayName":"Jay ra1","userId":"06251653160264065784"}}},"outputs":[],"source":["# Set seed for reproducibility\n","def set_seed(seed=42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","set_seed()\n","\n","# Initialize experiment tracking\n","def init_wandb(project_name, experiment_name, config):\n","    return wandb.init(\n","        # entity=\"dl_project_sp25\",\n","        project=project_name,\n","        name=experiment_name,\n","        config=config,\n","        reinit=True\n","    )\n","\n","# Load and prepare dataset\n","def prepare_dataset(dataset_name, image_processor, row_indx):\n","    \"\"\"\n","    Load and prepare a dataset from Hugging Face for ViT fine-tuning\n","    \"\"\"\n","    # Load the dataset\n","    print(f\"Loading dataset: {dataset_name}\")\n","    dataset = load_dataset(dataset_name)\n","\n","    # Get label information\n","    if \"label\" in dataset[\"train\"].features:\n","        labels = dataset[\"train\"].features[\"label\"].names\n","    elif \"labels\" in dataset[\"train\"].features:\n","        labels = dataset[\"train\"].features[\"labels\"].names\n","    else:\n","        # Count unique labels and create labels list\n","        all_labels = dataset[\"train\"][0][\"label\"] if \"label\" in dataset[\"train\"][0] else dataset[\"train\"][0][\"labels\"]\n","        num_labels = len(set(all_labels))\n","        labels = [str(i) for i in range(num_labels)]\n","\n","    # Create label mappings\n","    label2id = {label: i for i, label in enumerate(labels)}\n","    id2label = {i: label for i, label in enumerate(labels)}\n","\n","    # Set up image transformations based on the model's requirements\n","    normalize = transforms.Normalize(\n","        mean=image_processor.image_mean,\n","        std=image_processor.image_std\n","    )\n","\n","    # Get the expected image size\n","    if \"shortest_edge\" in image_processor.size:\n","        size = image_processor.size[\"shortest_edge\"]\n","    else:\n","        size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n","\n","    # Define transforms for training data\n","    train_transforms = transforms.Compose([\n","        transforms.RandomResizedCrop(size),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        normalize,\n","    ])\n","\n","    # Define transforms for validation/test data\n","    val_transforms = transforms.Compose([\n","        transforms.Resize(size),\n","        transforms.CenterCrop(size),\n","        transforms.ToTensor(),\n","        normalize,\n","    ])\n","\n","    # Apply transformations to the dataset\n","    def preprocess_train(examples):\n","        examples[\"pixel_values\"] = [\n","            train_transforms(image.convert(\"RGB\"))\n","            for image in examples[\"image\"]\n","        ]\n","        return examples\n","\n","    def preprocess_val(examples):\n","        examples[\"pixel_values\"] = [\n","            val_transforms(image.convert(\"RGB\"))\n","            for image in examples[\"image\"]\n","        ]\n","        return examples\n","\n","    # taking the image out for attention_mp\n","    #####Adding the row index for the image you want to get the attention map for\n","    data_for_img = dataset[\"train\"][row_indx]\n","\n","    # Apply preprocessing to each split\n","    train_dataset = dataset[\"train\"].map(\n","        preprocess_train, batched=True, remove_columns=[\"image\"]\n","    )\n","\n","    if \"validation\" in dataset:\n","        val_dataset = dataset[\"validation\"].map(\n","            preprocess_val, batched=True, remove_columns=[\"image\"]\n","        )\n","\n","    else:\n","        # Create a validation split if none exists\n","        splits = train_dataset.train_test_split(test_size=0.2, seed=42)\n","        train_dataset = splits[\"train\"]\n","        val_dataset = splits[\"test\"]\n","\n","    if \"test\" in dataset:\n","        test_dataset = dataset[\"test\"].map(\n","            preprocess_val, batched=True, remove_columns=[\"image\"]\n","        )\n","    else:\n","        # test_dataset = val_dataset    #split further rather than using validation as test dataset\n","\n","        # Further split validation dataset to create a test dataset\n","        test_split = val_dataset.train_test_split(test_size=0.2, seed=42)\n","        val_dataset = test_split[\"train\"]  # Update validation dataset\n","        test_dataset = test_split[\"test\"]  # Create test dataset\n","\n","    print(f\"Dataset prepared with {len(train_dataset)} training, {len(val_dataset)} validation, and {len(test_dataset)} test examples\")\n","\n","    return train_dataset, val_dataset, test_dataset, id2label, label2id, data_for_img\n","\n","# Define compute_metrics function for evaluation\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n","    acc = accuracy_score(labels, predictions)\n","\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall,\n","    }\n","\n","# Main experiment pipeline\n","def run_vit_experiment(config):\n","    \"\"\"\n","    Run a ViT experiment with the specified configuration\n","    \"\"\"\n","    # Initialize wandb for experiment tracking\n","    # run = init_wandb(\"ViT-LR-Schedulers\", config[\"experiment_name\"], config)\n","\n","    # Initialize wandb for experiment tracking with config logging\n","    run = wandb.init(\n","        project=\"ViT-LR-Schedulers\",\n","        name=config[\"experiment_name\"],\n","        group=f\"{config['optimizer_name']}_experiments\",  # Group by optimizer\n","        config={\n","            # Explicitly list all important hyperparameters\n","            \"optimizer\": config[\"optimizer_name\"],\n","            \"scheduler\": config[\"scheduler_name\"],\n","            \"learning_rate\": config[\"learning_rate\"],\n","            \"batch_size\": config[\"batch_size\"],\n","            \"num_epochs\": config[\"num_epochs\"],\n","            \"weight_decay\": config[\"weight_decay\"],\n","            \"warmup_ratio\": config.get(\"warmup_ratio\", 0.0),\n","            \"dataset\": config[\"dataset_name\"],\n","            \"model\": config[\"model_name\"],\n","            \"row_indx\" : config[\"row_indx\"],\n","            \"attention_steps\": config[\"attention_steps\"]\n","        },\n","        tags=[tag for tag in [config[\"optimizer_name\"], config[\"scheduler_name\"]] if tag is not None],\n","        reinit=True\n","    )\n","\n","\n","    # Load the image processor for the model\n","    image_processor = AutoImageProcessor.from_pretrained(config[\"model_name\"], use_fast=True)\n","\n","    # Prepare the dataset\n","    train_dataset, val_dataset, test_dataset, id2label, label2id, data_for_img = prepare_dataset(\n","        config[\"dataset_name\"], image_processor, config[\"row_indx\"]\n","    )\n","\n","    # Load the ViT model\n","    model = ViTForImageClassification.from_pretrained(\n","        config[\"model_name\"],\n","        num_labels=len(id2label),\n","        id2label=id2label,\n","        label2id=label2id,\n","        ignore_mismatched_sizes=True\n","    )\n","\n","    # Define training arguments\n","    training_args = TrainingArguments(\n","        output_dir=f\"./results/{config['experiment_name']}\",\n","        per_device_train_batch_size=config[\"batch_size\"],\n","        per_device_eval_batch_size=config[\"batch_size\"],\n","        num_train_epochs=config[\"num_epochs\"],\n","        weight_decay=config[\"weight_decay\"],\n","        eval_strategy=\"steps\",\n","        save_strategy=\"steps\",\n","        logging_strategy=\"steps\",  # Ensure logging is enabled\n","        logging_steps=10,          # Log every 10 steps (adjust as needed)\n","        load_best_model_at_end=True,\n","        metric_for_best_model=\"accuracy\",\n","        push_to_hub=False,\n","        report_to=\"wandb\",\n","        remove_unused_columns=False,\n","        learning_rate=config[\"learning_rate\"],\n","    )\n","\n","    # Setup optimizer\n","    # if config[\"optimizer_name\"] == \"AdamW\":\n","    #     optimizer = AdamW(model.parameters(), lr=config[\"learning_rate\"])\n","    # else:  # SGD\n","    #     optimizer = SGD(model.parameters(), lr=config[\"learning_rate\"], momentum=0.9)\n","\n","    # Full run, 10 epoch set up.\n","    if config[\"optimizer_name\"] == \"schedule_free_adamw\":\n","        optimizer = AdamWScheduleFree(model.parameters(), lr=config[\"learning_rate\"])\n","        scheduler = None  # No external scheduler\n","    elif config[\"optimizer_name\"] == \"AdamW\":\n","        optimizer = AdamW(model.parameters(), lr=config[\"learning_rate\"])\n","    elif config[\"optimizer_name\"] == \"SGD\":\n","        optimizer = SGD(model.parameters(), lr=config[\"learning_rate\"], momentum=0.9)\n","    elif config[\"optimizer_name\"] == \"RMSProp\":\n","        optimizer = torch.optim.RMSprop(model.parameters(), lr=config[\"learning_rate\"])\n","    elif config[\"optimizer_name\"] == \"AdaGrad\":\n","        optimizer = torch.optim.Adagrad(model.parameters(), lr=config[\"learning_rate\"])\n","    else:\n","        raise ValueError(f\"Optimizer {config['optimizer_name']} not supported\")\n","\n","    # Setup scheduler\n","    num_training_steps = len(train_dataset) // config[\"batch_size\"] * config[\"num_epochs\"]\n","    num_warmup_steps = int(num_training_steps * config[\"warmup_ratio\"]) if \"warmup_ratio\" in config else 0\n","\n","    scheduler_name = config[\"scheduler_name\"]\n","\n","    if scheduler_name is None:\n","        scheduler = None    # Handle the case for schedule_free\n","\n","    elif scheduler_name == \"linear\":\n","        scheduler = get_scheduler(\n","            \"linear\",\n","            optimizer=optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps\n","        )\n","    elif scheduler_name == \"cosine\":\n","        scheduler = get_scheduler(\n","            \"cosine\",\n","            optimizer=optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps\n","        )\n","\n","    elif scheduler_name == \"polynomial\":\n","        scheduler = get_scheduler(\n","            \"polynomial\",\n","            optimizer=optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps,\n","            # power=config.get(\"poly_power\", 1.0),\n","        )\n","    elif scheduler_name == \"cyclic\":\n","        # CyclicLR from torch.optim.lr_scheduler\n","        # Using step_size_up as 1/3 of training steps and step_size_down as 2/3\n","        step_size_up = num_training_steps // 3\n","        scheduler = CyclicLR(\n","            optimizer,\n","            base_lr=config[\"learning_rate\"] / 10,  # Lower bound of cycle\n","            max_lr=config[\"learning_rate\"],       # Upper bound of cycle\n","            step_size_up=step_size_up,\n","            step_size_down=step_size_up * 2,\n","            mode='triangular',                    # Three modes: triangular, triangular2, exp_range\n","            cycle_momentum=False                  # Don't cycle momentum\n","        )\n","    elif scheduler_name == \"exponential\":\n","        # ExponentialLR from torch.optim.lr_scheduler\n","        # gamma < 1.0 for decay, common values: 0.9, 0.95, 0.99\n","        scheduler = ExponentialLR(optimizer, gamma=0.95)\n","\n","    # elif scheduler_name == \"adaptive\":\n","    #     # ReduceLROnPlateau - reduces LR when metric stops improving\n","    #     # This requires modification to the training loop to update based on validation performance\n","    #     scheduler = ReduceLROnPlateau(\n","    #         optimizer,\n","    #         mode='max',              # Since we want to maximize accuracy\n","    #         factor=0.5,              # Multiply LR by this factor when plateauing\n","    #         patience=2,              # Number of epochs with no improvement after which LR will be reduced\n","    #         threshold=0.01,          # Threshold for measuring improvement\n","    #         threshold_mode='rel',    # Interpret threshold as relative change\n","    #         min_lr=1e-6              # Lower bound on the learning rate\n","    #     )\n","\n","    elif scheduler_name == \"constant\":\n","        scheduler = get_scheduler(\n","            \"constant\",\n","            optimizer=optimizer,\n","        )\n","    elif scheduler_name == \"cosine_with_restarts\":\n","        scheduler = get_scheduler(\n","            \"cosine_with_restarts\",\n","            optimizer=optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps,\n","        )\n","    elif scheduler_name == \"constant_with_warmup\":\n","        scheduler = get_scheduler(\n","            \"constant_with_warmup\",\n","            optimizer=optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","        )\n","    # add more experiments if required ...\n","\n","    else:\n","        raise ValueError(f\"Scheduler {scheduler_name} not supported\")\n","    # print(config)\n","    # print(config.keys())\n","    # print(config.optimizer_name)\n","    # Initialize attention callback (self, feature_extractor, image, output_dir,total_epochs,steps_tune, device = 'cuda'):\n","    # print(config['optimizer_name'])\n","    # print(scheduler_name)\n","    # print(config[\"attention_steps\"])\n","    # plt.imshow(data_for_img['image'])\n","    attn_map_clbck = Attentionmapcallback(image_processor,\n","                                  data_for_img['image'],\n","                                  './attention_maps/' + str(scheduler_name).split('.')[-1] + \"_\" + str(config['optimizer_name']).split('.')[-1],\n","                                  training_args.num_train_epochs,\n","                                  steps_tune = config[\"attention_steps\"])\n","\n","    # Initialize Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=val_dataset,\n","        compute_metrics=compute_metrics,\n","        optimizers=(optimizer, scheduler),\n","        callbacks = [attn_map_clbck]\n","\n","    )\n","\n","    # # Loss validation curve in the training loop to log metrics to W&B\n","    # for epoch in range(config[\"num_epochs\"]):\n","    #     print(f\"Epoch {epoch + 1}/{config['num_epochs']}\")\n","\n","    #     # Train for one epoch\n","    #     trainer.train()\n","\n","    #     # Evaluate on validation set\n","    #     eval_results = trainer.evaluate(val_dataset)\n","\n","    #     print(trainer.state.log_history)\n","    #     if trainer.state.log_history and \"loss\" in trainer.state.log_history[-1]:\n","    #         train_loss = trainer.state.log_history[-1][\"loss\"]\n","    #     else:\n","    #         train_loss = None\n","\n","    #     # Log training and validation metrics to W&B\n","    #     wandb.log({\n","    #         \"epoch\": epoch + 1,\n","    #         \"train_loss\": trainer.state.log_history[-1].get(\"loss\", None),\n","    #         \"val_loss\": eval_results[\"eval_loss\"],\n","    #         \"val_accuracy\": eval_results[\"eval_accuracy\"],\n","    #     })\n","\n","    # # Loss epoch curve in the training loop to log metrics to W&B\n","    # for epoch in range(config[\"num_epochs\"]):\n","    #     print(f\"Epoch {epoch + 1}/{config['num_epochs']}\")\n","\n","    #     # Train for one epoch\n","    #     trainer.train()\n","\n","    #     # Evaluate on validation set\n","    #     eval_results = trainer.evaluate(val_dataset)\n","\n","    #     # Extract training loss from the trainer's state\n","    #     if trainer.state.log_history and \"loss\" in trainer.state.log_history[-1]:\n","    #         train_loss = trainer.state.log_history[-1][\"loss\"]\n","    #     else:\n","    #         train_loss = None  # Handle missing loss gracefully\n","\n","    #     # Log training and validation metrics to W&B\n","    #     wandb.log({\n","    #         \"epoch\": epoch + 1,\n","    #         \"train_loss\": train_loss,                  # Training loss\n","    #         \"val_loss\": eval_results[\"eval_loss\"],    # Validation loss\n","    #         \"val_accuracy\": eval_results[\"eval_accuracy\"],  # Validation accuracy\n","    #     })\n","\n","    # Train the model\n","    print(f\"Starting training for {config['experiment_name']}...\")\n","    trainer.train()\n","\n","    # Evaluate the model\n","    print(f\"Evaluating {config['experiment_name']}...\")\n","    eval_results = trainer.evaluate(test_dataset)\n","\n","\n","    # Log final metrics\n","    wandb.log({\n","        \"final_accuracy\": eval_results[\"eval_accuracy\"],\n","        \"final_f1\": eval_results[\"eval_f1\"],\n","        \"final_precision\": eval_results[\"eval_precision\"],\n","        \"final_recall\": eval_results[\"eval_recall\"],\n","    })\n","\n","    # Compute confusion matrix for test set\n","    predictions, labels, _ = trainer.predict(test_dataset)\n","    predictions = np.argmax(predictions, axis=1)\n","\n","    # Convert to lists\n","    labels = labels.tolist()\n","    predictions = predictions.tolist()\n","\n","    # Log confusion matrix to W&B\n","    wandb.log({\n","        \"confusion_matrix_test\": wandb.plot.confusion_matrix(\n","            probs=None,\n","            y_true=labels,\n","            preds=predictions,\n","            class_names=[str(i) for i in range(len(np.unique(labels)))]\n","        )\n","    })\n","\n","\n","\n","    # Save the model\n","    trainer.save_model(f\"./saved_models/{config['experiment_name']}\")\n","\n","    # logging attention maps\n","    make_log_fig(config['attention_steps'], config['num_epochs'], config['scheduler_name'], config['optimizer_name'])\n","\n","    # Finish wandb run\n","    # wandb.finish()\n","\n","    return eval_results\n","\n","# Get experiment configurations for challenging datasets\n","def get_experiment_configs():\n","    # We'll use a more complex dataset from Hugging Face\n","    base_config = {\n","        \"model_name\": \"google/vit-base-patch16-224-in21k\",\n","        \"dataset_name\": \"jbarat/plant_species\",  # Any challenging dataset.\n","        \"batch_size\": 16,\n","        \"num_epochs\": 10, # let's keep smaller number to begin with.\n","        \"weight_decay\": 0.01,\n","        \"attention_steps\" : [5, 10, 15, 50,100,200,300,400,500],\n","        \"row_indx\" : 60\n","        # \"optimizer_name\": \"AdamW\",\n","    }\n","\n","    # Optimizers with their corresponding learning rates\n","    optimizers = {\n","        # \"schedule_free_adamw\": 0.0002,    #Lets get this separately.\n","        \"AdamW\": 0.0002,\n","        \"RMSProp\": 0.0002,\n","        \"AdaGrad\": 0.0002,\n","        \"SGD\": 0.02,\n","    }\n","\n","    # Schedulers to test\n","    schedulers = [\n","        \"linear\",\n","        \"cosine\",\n","        \"polynomial\",\n","        \"cyclic\",\n","        \"exponential\",\n","        # \"adaptive\",\n","        \"constant\",\n","        \"cosine_with_restarts\",\n","        \"constant_with_warmup\",\n","\n","    ]\n","\n","\n","\n","    # Different learning rate scheduler configurations\n","    configs = []\n","\n","    # # Constant learning rate (baseline)\n","    # configs.append({\n","    #     **base_config,\n","    #     \"experiment_name\": \"vit_constant_lr\",\n","    #     \"learning_rate\": 2e-4,\n","    #     \"scheduler_name\": \"constant\",\n","    # })\n","\n","    # # Add a schedule-free-only experiment\n","    # configs.append({\n","    #     **base_config,\n","    #     \"experiment_name\": \"schedule_free_adamw_no_scheduler\",\n","    #     \"optimizer_name\": \"schedule_free_adamw\",\n","    #     \"learning_rate\": 0.0002,\n","    #     \"scheduler_name\": None,  # Explicitly set to None\n","    # })\n","\n","\n","    for optimizer_name, learning_rate in optimizers.items():\n","        for scheduler_name in schedulers:\n","            config = {\n","                **base_config,\n","                \"experiment_name\": f\"{optimizer_name}_{scheduler_name}\",\n","                \"optimizer_name\": optimizer_name,\n","                \"learning_rate\": learning_rate,\n","                \"scheduler_name\": scheduler_name,\n","                \"warmup_ratio\": 0.1,  # Keep warmup ratio consistent\n","            }\n","            configs.append(config)\n","\n","\n","    # # Cosine with restarts\n","    # configs.append({\n","    #     **base_config,\n","    #     \"experiment_name\": \"vit_cosine_restarts\",\n","    #     \"learning_rate\": 2e-4,\n","    #     \"scheduler_name\": \"cosine_with_restarts\",\n","    #     \"warmup_ratio\": 0.1,\n","    # })\n","\n","    # # Constant with warmup\n","    # configs.append({\n","    #     **base_config,\n","    #     \"experiment_name\": \"vit_constant_warmup\",\n","    #     \"learning_rate\": 2e-4,\n","    #     \"scheduler_name\": \"constant_with_warmup\",\n","    #     \"warmup_ratio\": 0.1,\n","    # })\n","\n","    # # Linear decay\n","    # configs.append({\n","    #     **base_config,\n","    #     \"experiment_name\": \"vit_linear_decay\",\n","    #     \"learning_rate\": 5e-5,\n","    #     \"scheduler_name\": \"linear\",\n","    #     \"warmup_ratio\": 0.1,\n","    # })\n","\n","    # # Cosine decay (commonly used with ViT)\n","    # configs.append({\n","    #     **base_config,\n","    #     \"experiment_name\": \"vit_cosine_decay\",\n","    #     \"learning_rate\": 5e-5,\n","    #     \"scheduler_name\": \"cosine\",\n","    #     \"warmup_ratio\": 0.1,\n","    # })\n","\n","\n","    # # Polynomial decay\n","    # configs.append({\n","    #     **base_config,\n","    #     \"experiment_name\": \"vit_polynomial\",\n","    #     \"learning_rate\": 5e-5,\n","    #     \"scheduler_name\": \"polynomial\",\n","    #     \"warmup_ratio\": 0.1,\n","    #     \"poly_power\": 2.0,\n","    # })\n","\n","\n","    # # Different learning rate experiments\n","    # for lr in [1e-5, 3e-5, 1e-4]:\n","    #     configs.append({\n","    #         **base_config,\n","    #         \"experiment_name\": f\"vit_cosine_lr_{lr}\",\n","    #         \"learning_rate\": lr,\n","    #         \"scheduler_name\": \"cosine\",\n","    #         \"warmup_ratio\": 0.1,\n","    #     })\n","\n","    # # Different optimizer experiments\n","    # configs.append({\n","    #     **base_config,\n","    #     \"experiment_name\": \"vit_sgd_cosine\",\n","    #     \"learning_rate\": 0.01,  # Higher LR for SGD\n","    #     \"scheduler_name\": \"cosine\",\n","    #     \"warmup_ratio\": 0.1,\n","    #     \"optimizer_name\": \"SGD\",\n","    # })\n","\n","    # here we can make changes to add new datasets to experiment.\n","    # or change batch_size to see the impact.\n","    # Other datasets to try (uncomment to use)\n","    #   Erik: We can use a data set as a strech. Maybe something less similar than plants for better contrasting comparison?\n","    # flowers dataset\n","    # configs.append({\n","    #     **base_config,\n","    #     \"dataset_name\": \"huggan/flowers\",\n","    #     \"experiment_name\": \"vit_flowers_cosine\",\n","    #     \"learning_rate\": 5e-5,\n","    #     \"scheduler_name\": \"cosine\",\n","    #     \"warmup_ratio\": 0.1,\n","    # })\n","\n","    return configs\n","\n","# Run experiments and visualize results\n","def run_all_experiments():\n","    configs = get_experiment_configs()\n","    results = []\n","\n","    for config in configs:\n","        print(f\"\\n{'='*50}\")\n","        print(f\"Running experiment: {config['experiment_name']}\")\n","        print(f\"{'='*50}\\n\")\n","\n","        eval_results = run_vit_experiment(config)\n","        results.append({\n","            \"experiment\": config['experiment_name'],\n","            \"accuracy\": eval_results[\"eval_accuracy\"],\n","            \"f1\": eval_results[\"eval_f1\"],\n","            \"precision\": eval_results[\"eval_precision\"],\n","            \"recall\": eval_results[\"eval_recall\"],\n","            \"config\": config\n","        })\n","\n","    # Make sure to close the final run\n","    if wandb.run is not None:\n","        wandb.finish()\n","\n","    return results\n","\n","# Visualize and compare results\n","def visualize_results(results):\n","    # Create DataFrame for easier plotting\n","    df = pd.DataFrame([\n","        {\n","            \"Experiment\": result[\"experiment\"],\n","            \"Accuracy\": result[\"accuracy\"],\n","            \"F1 Score\": result[\"f1\"],\n","            \"Precision\": result[\"precision\"],\n","            \"Recall\": result[\"recall\"],\n","            \"Learning Rate\": result[\"config\"][\"learning_rate\"],\n","            \"Scheduler\": result[\"config\"][\"scheduler_name\"],\n","            \"Optimizer\": result[\"config\"][\"optimizer_name\"],\n","            \"Dataset\": result[\"config\"][\"dataset_name\"]\n","        }\n","        for result in results\n","    ])\n","\n","    # Plot accuracy comparison\n","    plt.figure(figsize=(14, 8))\n","    ax = plt.bar(df[\"Experiment\"], df[\"Accuracy\"], color='skyblue')\n","    plt.xlabel('Experiment')\n","    plt.ylabel('Accuracy')\n","    plt.title('Comparison of Model Accuracy Across Experiments')\n","    plt.xticks(rotation=45, ha='right')\n","    plt.tight_layout()\n","    plt.savefig(\"accuracy_comparison.png\")\n","    plt.close()\n","\n","    # Plot all metrics for a more comprehensive comparison\n","    plt.figure(figsize=(16, 10))\n","    metrics = [\"Accuracy\", \"F1 Score\", \"Precision\", \"Recall\"]\n","    x = np.arange(len(df[\"Experiment\"]))\n","    width = 0.2\n","\n","    for i, metric in enumerate(metrics):\n","        plt.bar(x + i*width, df[metric], width=width, label=metric)\n","\n","    plt.xlabel('Experiment')\n","    plt.ylabel('Score')\n","    plt.title('Comparison of Metrics Across Experiments')\n","    plt.xticks(x + width*1.5, df[\"Experiment\"], rotation=45, ha='right')\n","    plt.legend()\n","    plt.tight_layout()\n","    plt.savefig(\"metrics_comparison.png\")\n","    plt.close()\n","\n","    # Plot results by scheduler type\n","    plt.figure(figsize=(14, 8))\n","    schedulers = df[\"Scheduler\"].unique()\n","    for scheduler in schedulers:\n","        scheduler_data = df[df[\"Scheduler\"] == scheduler]\n","        plt.plot(scheduler_data[\"Learning Rate\"], scheduler_data[\"Accuracy\"], 'o-', label=scheduler)\n","\n","    plt.xlabel('Learning Rate')\n","    plt.ylabel('Accuracy')\n","    plt.title('Accuracy vs. Learning Rate by Scheduler Type')\n","    plt.xscale('log')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.tight_layout()\n","    plt.savefig(\"scheduler_comparison.png\")\n","    plt.close()\n","\n","    # Create a table with results\n","    print(\"Results Summary:\")\n","    print(df[[\"Experiment\", \"Accuracy\", \"F1 Score\", \"Precision\", \"Recall\", \"Scheduler\", \"Learning Rate\", \"Optimizer\", \"Dataset\"]])\n","\n","    # Save results to CSV\n","    df.to_csv(\"experiment_results.csv\", index=False)\n","\n","    return df\n","\n","# Function to run a single experiment (useful for testing)\n","def run_single_experiment(experiment_index=0):\n","    configs = get_experiment_configs()\n","    if experiment_index >= len(configs):\n","        print(f\"Invalid experiment index. Choose between 0 and {len(configs)-1}\")\n","        return\n","\n","    config = configs[experiment_index]\n","    print(f\"Running single experiment: {config['experiment_name']}\")\n","    eval_results = run_vit_experiment(config)\n","\n","    print(f\"\\nResults for {config['experiment_name']}:\")\n","    print(f\"Accuracy: {eval_results['eval_accuracy']:.4f}\")\n","    print(f\"F1 Score: {eval_results['eval_f1']:.4f}\")\n","    print(f\"Precision: {eval_results['eval_precision']:.4f}\")\n","    print(f\"Recall: {eval_results['eval_recall']:.4f}\")\n","\n","    return eval_results\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"0k1GA2B6fDnw","executionInfo":{"status":"ok","timestamp":1745212854013,"user_tz":240,"elapsed":101,"user":{"displayName":"Jay ra1","userId":"06251653160264065784"}}},"outputs":[],"source":["# # Uncomment to use: For 3 epoch sweep with only cosine scheduler to get best LR:\n","# def run_optimizer_sweep():\n","#     # Initialize W&B run first, then access config\n","#     with wandb.init() as run:\n","#         print(f\"W&B initialized: {run.name}\")\n","\n","#         # Get config from sweep\n","#         config = wandb.config\n","\n","#         # Set experiment name based on sweep parameters\n","#         custom_name = f\"vit_{config.optimizer_name}_{config.learning_rate}\"\n","#         # Update the run name after initialization\n","#         wandb.run.name = custom_name\n","#         wandb.run.save()\n","\n","#         print(f\"Running experiment: {custom_name}\")\n","\n","#         # Load model and processor\n","#         model_name = \"google/vit-base-patch16-224-in21k\"\n","#         dataset_name = \"jbarat/plant_species\"\n","\n","#         # Load the image processor\n","#         image_processor = AutoImageProcessor.from_pretrained(model_name, use_fast=True)\n","\n","#         # Prepare dataset\n","#         train_dataset, val_dataset, test_dataset, id2label, label2id, data_for_img = prepare_dataset(\n","#             dataset_name, image_processor, config.row_indx\n","#         )\n","\n","#         # Load the ViT model\n","#         model = ViTForImageClassification.from_pretrained(\n","#             model_name,\n","#             num_labels=len(id2label),\n","#             id2label=id2label,\n","#             label2id=label2id,\n","#             ignore_mismatched_sizes=True\n","#         )\n","\n","#         # Define training arguments\n","#         training_args = TrainingArguments(\n","#             output_dir=f\"./results/{custom_name}\",\n","#             per_device_train_batch_size=config.batch_size,\n","#             per_device_eval_batch_size=config.batch_size,\n","#             num_train_epochs=config.num_epochs,\n","#             weight_decay=0.01,\n","#             eval_strategy=\"steps\",\n","#             save_strategy=\"steps\",\n","#             logging_strategy=\"steps\",\n","#             logging_steps=10,\n","#             load_best_model_at_end=True,\n","#             metric_for_best_model=\"accuracy\",\n","#             push_to_hub=False,\n","#             report_to=\"wandb\",\n","#             remove_unused_columns=False,\n","#             learning_rate=config.learning_rate,\n","#         )\n","#         # Set up optimizer based on config\n","#         if config.optimizer_name == \"schedule_free_adamw\":\n","#             optimizer = AdamWScheduleFree(\n","#                 model.parameters(),\n","#                 lr=config.learning_rate,  # Learning rate\n","#                 # warmup_steps=500  # Optional: Adjust based on your dataset\n","#             )\n","#         elif config.optimizer_name == \"AdamW\":\n","#             optimizer = AdamW(model.parameters(), lr=config.learning_rate)\n","#         elif config.optimizer_name == \"SGD\":\n","#             optimizer = SGD(model.parameters(), lr=config.learning_rate, momentum=0.9)\n","#         elif config.optimizer_name == \"RMSProp\":\n","#             optimizer = torch.optim.RMSprop(model.parameters(), lr=config.learning_rate)\n","#         elif config.optimizer_name == \"AdaGrad\":\n","#             optimizer = torch.optim.Adagrad(model.parameters(), lr=config.learning_rate)\n","#         else:\n","#             optimizer = AdamW(model.parameters(), lr=config.learning_rate)\n","\n","#         # Setup scheduler\n","#         num_training_steps = len(train_dataset) // config.batch_size * config.num_epochs\n","#         num_warmup_steps = int(num_training_steps * 0.1)  # 10% warmup\n","\n","#         scheduler = get_scheduler(\n","#             config.scheduler_name,\n","#             optimizer=optimizer,\n","#             num_warmup_steps=num_warmup_steps,\n","#             num_training_steps=num_training_steps\n","#         )\n","\n","\n","#         # Initialize attention callback (self, feature_extractor, image, output_dir,total_epochs,steps_tune, device = 'cuda'):\n","#         attn_map_clbck = Attentionmapcallback(image_processor,\n","#                                       data_for_img['image'],\n","#                                       './attention_maps/' + str(config.scheduler_name).split('.')[-1] + \"_\" + str(config.optimizer_name).split('.')[-1],\n","#                                       training_args.num_train_epochs,\n","#                                       steps_tune = config.attention_steps)\n","\n","\n","#         # Initialize Trainer\n","#         trainer = Trainer(\n","#             model=model,\n","#             args=training_args,\n","#             train_dataset=train_dataset,\n","#             eval_dataset=val_dataset,\n","#             compute_metrics=compute_metrics,\n","#             optimizers=(optimizer, scheduler),\n","#             callbacks = [attn_map_clbck]\n","#         )\n","\n","#         # Train the model\n","#         print(f\"Starting training...\")\n","#         # optimizer.train()  # Switch optimizer to training mode only for schedule_free\n","#         trainer.train()\n","\n","#         # Evaluate on validation dataset\n","#         print(f\"Evaluating on validation set...\")\n","#         # optimizer.eval()  # Switch optimizer to evaluation mode only for schedule_free\n","#         eval_results = trainer.evaluate(val_dataset)\n","\n","#         # Log validation metrics\n","#         run.log({\n","#             \"val_accuracy\": eval_results[\"eval_accuracy\"],\n","#             \"val_f1\": eval_results[\"eval_f1\"],\n","#             \"val_precision\": eval_results[\"eval_precision\"],\n","#             \"val_recall\": eval_results[\"eval_recall\"],\n","#             \"val_loss\": eval_results[\"eval_loss\"]\n","#         })\n","\n","#         # Evaluate on test dataset\n","#         print(f\"Evaluating on test set...\")\n","#         test_results = trainer.evaluate(test_dataset)\n","\n","#         # Log test metrics\n","#         run.log({\n","#             \"test_accuracy\": test_results[\"eval_accuracy\"],\n","#             \"test_f1\": test_results[\"eval_f1\"],\n","#             \"test_precision\": test_results[\"eval_precision\"],\n","#             \"test_recall\": test_results[\"eval_recall\"],\n","#             \"test_loss\": test_results[\"eval_loss\"]\n","#         })\n","\n","#         # Compute confusion matrix for test set\n","#         predictions, labels, _ = trainer.predict(test_dataset)\n","#         predictions = np.argmax(predictions, axis=1)\n","\n","#         # Log confusion matrix\n","#         run.log({\n","#             \"confusion_matrix\": wandb.plot.confusion_matrix(\n","#                 probs=None,\n","#                 y_true=labels.tolist(),\n","#                  preds=predictions.tolist(),\n","#                 class_names=[id2label[i] for i in range(len(id2label))]\n","#             )\n","#         })\n","\n","#         # Save the model\n","#         model_path = f\"./saved_models/{custom_name}\"\n","#         trainer.save_model(model_path)\n","#         print(f\"Model saved to {model_path}\")\n","\n","#         # Save the attn_fig\n","#         make_log_fig(config.attention_steps, config.num_epochs, config.scheduler_name, config.optimizer_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["e2e666523104497486d06bfe4138ad25","669d4b0fedca466799bd6ce41ebf011d","c0fc2f1ca99e4b84a2b1ac145819381a","0d8f77fea57347ca8dc34ff0fad06268","90b3bbd8082741f9993056593613f145","31f0d3232c0e4edd891b9594bf4a2cf3","3dbe2829bcb841dfb93a7ce6375f5a5a","a38fb36e8b3b40c79a93a15f17d72223","490b5d9904c0414ab68c21a93ed542a8","dc9173e7c7c047e3869265f3d96419a2","8b0b0df877824431aa37a25d8a934d6c","25b9b3283e75428faf20a5e114be85e5","30c8ca7e62834aa489f9c5472d67c0d3","b3c4007668e8452bb47d3b8242074332","2c01d9fdaa8548f5967dcf7651c1ff0d","08a060a10887476d93bc4abbab85056d","363199b2c23846158cca742e8fed1632","43b30eb6f03644a29d4de1a40cd050f5","d5b63b89489641f2897189ec3fedb8f4","32bac537713f422a8e09d472d34ab506","e87c86ce4b394c7880cfa36ffb4336b6","1a5924f8833f44acaab79343921d7617","b00860ce72b445a69128db2e4b8283c9","42d3c618f06048f5a28d7d3d103402f8","5b937f8b0b25470088f756c18c61c120","5cc99b41d48b4b0a913f107196062df5","3aa5c23c1ea149f0998ac04a03bc7ea1","f7c50bfc9c7a46bda6ab7d20bcece3db","03d5b16c841d451bae1d924e209c87ed","d927f80bacdb48f4986b3242be6e7cec","13af8afa4eb84e04af54971533f7b3e1","acdb8ee2b46c4b4b8f6f0380a683d1e1","e39e6b35a51e4a4f83b90c90e25ca324","239fc9b726d647879ec067e03f221c64","071739b15727463f87525c95e50b394c","04b018e2bccb420aab57e301cbccabfb","e5526b6b3a92450aa3e7d67e16e6c938","200239f89b6142ab83e99eac9177babf","99b715af4caa4cc49cabe55c30bf8238","03072dc66e78484e9145dc662081075b","1a34ced828a446d39553f871bb7664e9","a0cd8fb0fce34a0a8ee7709cb6a641e6","c8716a139c01441d9f542c8ae201c888","a44b125473b649218a6f90f3a95a7ed1","c914c14223184ed4be2699b71393f3d2","2802db3e2c764beea0ed0225dee78d1a","93345bde2245420b9e96558cbe1a02bf","6181ba93e4d34337b0db8cf63346702b","64fefbbe0a6143f89431aceb52aa0370","6c1177efdd064e1eb2758a56649baa48","a75957b876bf4a6698fda1103a63b3fd","7a62a6575c8b4fc79d722bdc5a586556","667d51c51d5d441693e90a8a3d1ba780","af309c1492254ed4826aff7937278d4d","ccb69ddeb5bb4c2598a4a0ce01ece067","45cfc02d67934147bc2ff1221156044d","6ce8b85280a84f4f869b4a1a3ac8065d","c336d703588b401d9e14abbc4ece3516","8eedeabe9b16431eb118302bd89e0e29","ec417519b40c4c229777c1aa667eee13","2a07ab51bcf5422abf39dea3a28b3594","e38ab7afe5df4f10a3d50fa389324161","0a8f3ed115404efa9dab9d57e35df821","865d9fcea7b74ec8bbd774fcee53a758","2d1aed8482694a02a78429ff27ab2fa0","dba42333ffd346319860aa8cf566f3f0","c5ee4e9474b04e5491ad16696dd6b1fe","c874013e0c8d41178939086db4c901af","e9995581f57c4297a1a09e5f3ba57f30","f8688fc91fd7474aad3011d4c7d4745a","6d31f05b5c3b47bf98e27063977ae2bf","e310a67f025e4097b51bbe9dc2c18241","c8d9bd26bd2a411784f5001da8f9bb30","387cc4b035cd4425b4c7b47b5d7aa163","4afa1e04021d4b13beb3fbabbe5ae8a3","ced200a5eca5457e9124749a452b1cb7","a84bb6c1fe134cc6ba81f180dd6039e9"]},"id":"AgLdeTtFtIgc","outputId":"0d9962d7-7c42-47b2-8e61-1479fef6b24b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting ViT experiments with different learning rate schedulers...\n","\n","==================================================\n","Running experiment: AdamW_linear\n","==================================================\n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjraisinghani3\u001b[0m (\u001b[33mdl_project_sp25\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.9"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250421_052057-db3payia</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/dl_project_sp25/ViT-LR-Schedulers/runs/db3payia' target=\"_blank\">AdamW_linear</a></strong> to <a href='https://wandb.ai/dl_project_sp25/ViT-LR-Schedulers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/dl_project_sp25/ViT-LR-Schedulers' target=\"_blank\">https://wandb.ai/dl_project_sp25/ViT-LR-Schedulers</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/dl_project_sp25/ViT-LR-Schedulers/runs/db3payia' target=\"_blank\">https://wandb.ai/dl_project_sp25/ViT-LR-Schedulers/runs/db3payia</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2e666523104497486d06bfe4138ad25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25b9b3283e75428faf20a5e114be85e5"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loading dataset: jbarat/plant_species\n"]},{"output_type":"display_data","data":{"text/plain":["README.md:   0%|          | 0.00/800 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b00860ce72b445a69128db2e4b8283c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["(…)-00000-of-00001-15efca0bf2e6a460.parquet:   0%|          | 0.00/82.0M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"239fc9b726d647879ec067e03f221c64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/800 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c914c14223184ed4be2699b71393f3d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/800 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45cfc02d67934147bc2ff1221156044d"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset prepared with 640 training, 128 validation, and 32 test examples\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5ee4e9474b04e5491ad16696dd6b1fe"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Starting training for AdamW_linear...\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [120/120 07:17, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>2.019100</td>\n","      <td>1.858745</td>\n","      <td>0.546875</td>\n","      <td>0.514830</td>\n","      <td>0.527753</td>\n","      <td>0.546875</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.694600</td>\n","      <td>1.442010</td>\n","      <td>0.687500</td>\n","      <td>0.651616</td>\n","      <td>0.747841</td>\n","      <td>0.687500</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.356000</td>\n","      <td>1.130157</td>\n","      <td>0.773438</td>\n","      <td>0.771510</td>\n","      <td>0.784385</td>\n","      <td>0.773438</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.044000</td>\n","      <td>0.990746</td>\n","      <td>0.734375</td>\n","      <td>0.734330</td>\n","      <td>0.765696</td>\n","      <td>0.734375</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.682400</td>\n","      <td>0.890501</td>\n","      <td>0.757812</td>\n","      <td>0.758126</td>\n","      <td>0.785176</td>\n","      <td>0.757812</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.627500</td>\n","      <td>0.780365</td>\n","      <td>0.789062</td>\n","      <td>0.788324</td>\n","      <td>0.818400</td>\n","      <td>0.789062</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.529000</td>\n","      <td>0.732681</td>\n","      <td>0.773438</td>\n","      <td>0.773331</td>\n","      <td>0.801166</td>\n","      <td>0.773438</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.509400</td>\n","      <td>0.660391</td>\n","      <td>0.796875</td>\n","      <td>0.796551</td>\n","      <td>0.810506</td>\n","      <td>0.796875</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.336000</td>\n","      <td>0.640551</td>\n","      <td>0.812500</td>\n","      <td>0.815499</td>\n","      <td>0.853582</td>\n","      <td>0.812500</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.294300</td>\n","      <td>0.643465</td>\n","      <td>0.804688</td>\n","      <td>0.801936</td>\n","      <td>0.821672</td>\n","      <td>0.804688</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.282400</td>\n","      <td>0.638664</td>\n","      <td>0.812500</td>\n","      <td>0.806112</td>\n","      <td>0.854410</td>\n","      <td>0.812500</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.265400</td>\n","      <td>0.615634</td>\n","      <td>0.820312</td>\n","      <td>0.818006</td>\n","      <td>0.831845</td>\n","      <td>0.820312</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n","/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Evaluating AdamW_linear...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","==================================================\n","Running experiment: AdamW_cosine\n","==================================================\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▅▇▆▆▇▇▇████▆</td></tr><tr><td>eval/f1</td><td>▁▄▇▆▇▇▇█████▆</td></tr><tr><td>eval/loss</td><td>█▆▄▃▃▂▂▁▁▁▁▁▂</td></tr><tr><td>eval/precision</td><td>▁▆▆▆▇▇▇▇█▇██▇</td></tr><tr><td>eval/recall</td><td>▁▅▇▆▆▇▇▇████▆</td></tr><tr><td>eval/runtime</td><td>▅▅▅▆▅▅▅▆▆▆▇█▁</td></tr><tr><td>eval/samples_per_second</td><td>███▇███▇▇▇▃▁▃</td></tr><tr><td>eval/steps_per_second</td><td>███▇███▇▇▇▃▁▃</td></tr><tr><td>final_accuracy</td><td>▁</td></tr><tr><td>final_f1</td><td>▁</td></tr><tr><td>final_precision</td><td>▁</td></tr><tr><td>final_recall</td><td>▁</td></tr><tr><td>test/accuracy</td><td>▁</td></tr><tr><td>test/f1</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>test/precision</td><td>▁</td></tr><tr><td>test/recall</td><td>▁</td></tr><tr><td>test/runtime</td><td>▁</td></tr><tr><td>test/samples_per_second</td><td>▁</td></tr><tr><td>test/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇████████</td></tr><tr><td>train/grad_norm</td><td>▃▅▅▆▂▄█▂▁▃▃▄</td></tr><tr><td>train/learning_rate</td><td>▇█▇▇▆▅▅▄▃▂▂▁</td></tr><tr><td>train/loss</td><td>█▇▅▄▃▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.75</td></tr><tr><td>eval/f1</td><td>0.7224</td></tr><tr><td>eval/loss</td><td>0.74217</td></tr><tr><td>eval/precision</td><td>0.82465</td></tr><tr><td>eval/recall</td><td>0.75</td></tr><tr><td>eval/runtime</td><td>4.2365</td></tr><tr><td>eval/samples_per_second</td><td>7.553</td></tr><tr><td>eval/steps_per_second</td><td>0.472</td></tr><tr><td>final_accuracy</td><td>0.75</td></tr><tr><td>final_f1</td><td>0.7224</td></tr><tr><td>final_precision</td><td>0.82465</td></tr><tr><td>final_recall</td><td>0.75</td></tr><tr><td>test/accuracy</td><td>0.75</td></tr><tr><td>test/f1</td><td>0.7224</td></tr><tr><td>test/loss</td><td>0.74217</td></tr><tr><td>test/precision</td><td>0.82465</td></tr><tr><td>test/recall</td><td>0.75</td></tr><tr><td>test/runtime</td><td>3.5676</td></tr><tr><td>test/samples_per_second</td><td>8.97</td></tr><tr><td>test/steps_per_second</td><td>0.561</td></tr><tr><td>total_flos</td><td>1.4879262111694848e+17</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>120</td></tr><tr><td>train/grad_norm</td><td>2.0707</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.2654</td></tr><tr><td>train_loss</td><td>0.80333</td></tr><tr><td>train_runtime</td><td>442.2777</td></tr><tr><td>train_samples_per_second</td><td>4.341</td></tr><tr><td>train_steps_per_second</td><td>0.271</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">AdamW_linear</strong> at: <a href='https://wandb.ai/dl_project_sp25/ViT-LR-Schedulers/runs/db3payia' target=\"_blank\">https://wandb.ai/dl_project_sp25/ViT-LR-Schedulers/runs/db3payia</a><br> View project at: <a href='https://wandb.ai/dl_project_sp25/ViT-LR-Schedulers' target=\"_blank\">https://wandb.ai/dl_project_sp25/ViT-LR-Schedulers</a><br>Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250421_052057-db3payia/logs</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.9"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250421_052903-sxmebu04</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/dl_project_sp25/ViT-LR-Schedulers/runs/sxmebu04' target=\"_blank\">AdamW_cosine</a></strong> to <a href='https://wandb.ai/dl_project_sp25/ViT-LR-Schedulers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/dl_project_sp25/ViT-LR-Schedulers' target=\"_blank\">https://wandb.ai/dl_project_sp25/ViT-LR-Schedulers</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/dl_project_sp25/ViT-LR-Schedulers/runs/sxmebu04' target=\"_blank\">https://wandb.ai/dl_project_sp25/ViT-LR-Schedulers/runs/sxmebu04</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loading dataset: jbarat/plant_species\n","Dataset prepared with 640 training, 128 validation, and 32 test examples\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Starting training for AdamW_cosine...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='43' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 43/120 02:29 < 04:40, 0.27 it/s, Epoch 1.05/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>2.023800</td>\n","      <td>1.832424</td>\n","      <td>0.671875</td>\n","      <td>0.661673</td>\n","      <td>0.663536</td>\n","      <td>0.671875</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.639500</td>\n","      <td>1.366410</td>\n","      <td>0.742188</td>\n","      <td>0.714578</td>\n","      <td>0.796885</td>\n","      <td>0.742188</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.295500</td>\n","      <td>1.087709</td>\n","      <td>0.789062</td>\n","      <td>0.790965</td>\n","      <td>0.801439</td>\n","      <td>0.789062</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.985400</td>\n","      <td>0.941136</td>\n","      <td>0.781250</td>\n","      <td>0.781186</td>\n","      <td>0.820797</td>\n","      <td>0.781250</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}}],"source":["\n","# Main execution\n","if __name__ == \"__main__\":\n","    print(\"Starting ViT experiments with different learning rate schedulers...\")\n","    os.environ[\"WANDB_PROJECT\"] = \"ViT-LR-Schedulers\"\n","\n","    # Option 1: Run all experiments (time-consuming)\n","    results = run_all_experiments()\n","    results_df = visualize_results(results)\n","\n","    # Option 2: Run a single experiment for testing\n","    # run_single_experiment(0)  # Try the baseline experiment first\n","\n","    #option 3: Optimizer sweep:\n","    # Define sweep configuration\n","    # sweep_config = {\n","    #     \"method\": \"grid\",  # we can use \"grid\", \"random\", or \"bayes\"\n","    #     \"metric\": {\n","    #         \"name\": \"val_accuracy\",  # Metric to optimize\n","    #         \"goal\": \"maximize\"       # Goal: maximize or minimize\n","    #     },\n","    #     \"parameters\": {\n","    #         \"optimizer_name\": {\n","    #             \"values\": [\"schedule_free_adamw\",\"AdamW\", \"SGD\", \"RMSProp\", \"AdaGrad\"]  # Optimizers to test\n","    #         },\n","    #         \"learning_rate\": {\n","    #             \"values\": [2e-5, 2e-4, 2e-3, 2e-2, 2e-1]  # Learning rates to test\n","    #         },\n","    #         \"batch_size\": {\n","    #             \"values\": [16]\n","    #         },\n","    #         \"num_epochs\": {\n","    #             \"values\": [3]\n","    #         },\n","    #         \"scheduler_name\": {\n","    #             \"values\": [\"cosine\"]\n","    #         }\n","    #     }\n","    # }\n","\n","    # # Initialize the sweep\n","    # sweep_id = wandb.sweep(sweep_config, project=\"ViT-Optimizer-Sweep\")\n","    # wandb.agent(sweep_id, function=run_optimizer_sweep)\n","\n","\n","    print(\"Experiments completed!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dON4fJvq0Ulv"},"outputs":[],"source":["# Identify best of learning rate sweep:\n","# import pandas as pd\n","\n","# # Load the exported CSV file\n","# df = pd.read_csv(\"wandb_export.csv\")\n","\n","# # Group by optimizer and find the best learning rate for each\n","# best_lr_per_optimizer = (\n","#     df.groupby(\"optimizer_name\")\n","#     .apply(lambda group: group.loc[group[\"val_accuracy\"].idxmax()])\n","#     [[\"optimizer_name\", \"learning_rate\", \"val_accuracy\"]]\n","# )\n","\n","# print(best_lr_per_optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OJqA_rlc0Ulv"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QjswKzxwfDnx"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"e2e666523104497486d06bfe4138ad25":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_669d4b0fedca466799bd6ce41ebf011d","IPY_MODEL_c0fc2f1ca99e4b84a2b1ac145819381a","IPY_MODEL_0d8f77fea57347ca8dc34ff0fad06268"],"layout":"IPY_MODEL_90b3bbd8082741f9993056593613f145"}},"669d4b0fedca466799bd6ce41ebf011d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_31f0d3232c0e4edd891b9594bf4a2cf3","placeholder":"​","style":"IPY_MODEL_3dbe2829bcb841dfb93a7ce6375f5a5a","value":"preprocessor_config.json: 100%"}},"c0fc2f1ca99e4b84a2b1ac145819381a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a38fb36e8b3b40c79a93a15f17d72223","max":160,"min":0,"orientation":"horizontal","style":"IPY_MODEL_490b5d9904c0414ab68c21a93ed542a8","value":160}},"0d8f77fea57347ca8dc34ff0fad06268":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc9173e7c7c047e3869265f3d96419a2","placeholder":"​","style":"IPY_MODEL_8b0b0df877824431aa37a25d8a934d6c","value":" 160/160 [00:00&lt;00:00, 6.33kB/s]"}},"90b3bbd8082741f9993056593613f145":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31f0d3232c0e4edd891b9594bf4a2cf3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3dbe2829bcb841dfb93a7ce6375f5a5a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a38fb36e8b3b40c79a93a15f17d72223":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"490b5d9904c0414ab68c21a93ed542a8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dc9173e7c7c047e3869265f3d96419a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b0b0df877824431aa37a25d8a934d6c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25b9b3283e75428faf20a5e114be85e5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_30c8ca7e62834aa489f9c5472d67c0d3","IPY_MODEL_b3c4007668e8452bb47d3b8242074332","IPY_MODEL_2c01d9fdaa8548f5967dcf7651c1ff0d"],"layout":"IPY_MODEL_08a060a10887476d93bc4abbab85056d"}},"30c8ca7e62834aa489f9c5472d67c0d3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_363199b2c23846158cca742e8fed1632","placeholder":"​","style":"IPY_MODEL_43b30eb6f03644a29d4de1a40cd050f5","value":"config.json: 100%"}},"b3c4007668e8452bb47d3b8242074332":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5b63b89489641f2897189ec3fedb8f4","max":502,"min":0,"orientation":"horizontal","style":"IPY_MODEL_32bac537713f422a8e09d472d34ab506","value":502}},"2c01d9fdaa8548f5967dcf7651c1ff0d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e87c86ce4b394c7880cfa36ffb4336b6","placeholder":"​","style":"IPY_MODEL_1a5924f8833f44acaab79343921d7617","value":" 502/502 [00:00&lt;00:00, 14.0kB/s]"}},"08a060a10887476d93bc4abbab85056d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"363199b2c23846158cca742e8fed1632":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43b30eb6f03644a29d4de1a40cd050f5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d5b63b89489641f2897189ec3fedb8f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32bac537713f422a8e09d472d34ab506":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e87c86ce4b394c7880cfa36ffb4336b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a5924f8833f44acaab79343921d7617":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b00860ce72b445a69128db2e4b8283c9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_42d3c618f06048f5a28d7d3d103402f8","IPY_MODEL_5b937f8b0b25470088f756c18c61c120","IPY_MODEL_5cc99b41d48b4b0a913f107196062df5"],"layout":"IPY_MODEL_3aa5c23c1ea149f0998ac04a03bc7ea1"}},"42d3c618f06048f5a28d7d3d103402f8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7c50bfc9c7a46bda6ab7d20bcece3db","placeholder":"​","style":"IPY_MODEL_03d5b16c841d451bae1d924e209c87ed","value":"README.md: 100%"}},"5b937f8b0b25470088f756c18c61c120":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d927f80bacdb48f4986b3242be6e7cec","max":800,"min":0,"orientation":"horizontal","style":"IPY_MODEL_13af8afa4eb84e04af54971533f7b3e1","value":800}},"5cc99b41d48b4b0a913f107196062df5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_acdb8ee2b46c4b4b8f6f0380a683d1e1","placeholder":"​","style":"IPY_MODEL_e39e6b35a51e4a4f83b90c90e25ca324","value":" 800/800 [00:00&lt;00:00, 27.2kB/s]"}},"3aa5c23c1ea149f0998ac04a03bc7ea1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7c50bfc9c7a46bda6ab7d20bcece3db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03d5b16c841d451bae1d924e209c87ed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d927f80bacdb48f4986b3242be6e7cec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13af8afa4eb84e04af54971533f7b3e1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"acdb8ee2b46c4b4b8f6f0380a683d1e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e39e6b35a51e4a4f83b90c90e25ca324":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"239fc9b726d647879ec067e03f221c64":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_071739b15727463f87525c95e50b394c","IPY_MODEL_04b018e2bccb420aab57e301cbccabfb","IPY_MODEL_e5526b6b3a92450aa3e7d67e16e6c938"],"layout":"IPY_MODEL_200239f89b6142ab83e99eac9177babf"}},"071739b15727463f87525c95e50b394c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_99b715af4caa4cc49cabe55c30bf8238","placeholder":"​","style":"IPY_MODEL_03072dc66e78484e9145dc662081075b","value":"(…)-00000-of-00001-15efca0bf2e6a460.parquet: 100%"}},"04b018e2bccb420aab57e301cbccabfb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a34ced828a446d39553f871bb7664e9","max":82004194,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a0cd8fb0fce34a0a8ee7709cb6a641e6","value":82004194}},"e5526b6b3a92450aa3e7d67e16e6c938":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8716a139c01441d9f542c8ae201c888","placeholder":"​","style":"IPY_MODEL_a44b125473b649218a6f90f3a95a7ed1","value":" 82.0M/82.0M [00:03&lt;00:00, 23.2MB/s]"}},"200239f89b6142ab83e99eac9177babf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99b715af4caa4cc49cabe55c30bf8238":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03072dc66e78484e9145dc662081075b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1a34ced828a446d39553f871bb7664e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0cd8fb0fce34a0a8ee7709cb6a641e6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c8716a139c01441d9f542c8ae201c888":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a44b125473b649218a6f90f3a95a7ed1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c914c14223184ed4be2699b71393f3d2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2802db3e2c764beea0ed0225dee78d1a","IPY_MODEL_93345bde2245420b9e96558cbe1a02bf","IPY_MODEL_6181ba93e4d34337b0db8cf63346702b"],"layout":"IPY_MODEL_64fefbbe0a6143f89431aceb52aa0370"}},"2802db3e2c764beea0ed0225dee78d1a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c1177efdd064e1eb2758a56649baa48","placeholder":"​","style":"IPY_MODEL_a75957b876bf4a6698fda1103a63b3fd","value":"Generating train split: 100%"}},"93345bde2245420b9e96558cbe1a02bf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a62a6575c8b4fc79d722bdc5a586556","max":800,"min":0,"orientation":"horizontal","style":"IPY_MODEL_667d51c51d5d441693e90a8a3d1ba780","value":800}},"6181ba93e4d34337b0db8cf63346702b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af309c1492254ed4826aff7937278d4d","placeholder":"​","style":"IPY_MODEL_ccb69ddeb5bb4c2598a4a0ce01ece067","value":" 800/800 [00:00&lt;00:00, 986.60 examples/s]"}},"64fefbbe0a6143f89431aceb52aa0370":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c1177efdd064e1eb2758a56649baa48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a75957b876bf4a6698fda1103a63b3fd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a62a6575c8b4fc79d722bdc5a586556":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"667d51c51d5d441693e90a8a3d1ba780":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"af309c1492254ed4826aff7937278d4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ccb69ddeb5bb4c2598a4a0ce01ece067":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"45cfc02d67934147bc2ff1221156044d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6ce8b85280a84f4f869b4a1a3ac8065d","IPY_MODEL_c336d703588b401d9e14abbc4ece3516","IPY_MODEL_8eedeabe9b16431eb118302bd89e0e29"],"layout":"IPY_MODEL_ec417519b40c4c229777c1aa667eee13"}},"6ce8b85280a84f4f869b4a1a3ac8065d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a07ab51bcf5422abf39dea3a28b3594","placeholder":"​","style":"IPY_MODEL_e38ab7afe5df4f10a3d50fa389324161","value":"Map: 100%"}},"c336d703588b401d9e14abbc4ece3516":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a8f3ed115404efa9dab9d57e35df821","max":800,"min":0,"orientation":"horizontal","style":"IPY_MODEL_865d9fcea7b74ec8bbd774fcee53a758","value":800}},"8eedeabe9b16431eb118302bd89e0e29":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d1aed8482694a02a78429ff27ab2fa0","placeholder":"​","style":"IPY_MODEL_dba42333ffd346319860aa8cf566f3f0","value":" 800/800 [00:16&lt;00:00, 47.53 examples/s]"}},"ec417519b40c4c229777c1aa667eee13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a07ab51bcf5422abf39dea3a28b3594":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e38ab7afe5df4f10a3d50fa389324161":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0a8f3ed115404efa9dab9d57e35df821":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"865d9fcea7b74ec8bbd774fcee53a758":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2d1aed8482694a02a78429ff27ab2fa0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dba42333ffd346319860aa8cf566f3f0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c5ee4e9474b04e5491ad16696dd6b1fe":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c874013e0c8d41178939086db4c901af","IPY_MODEL_e9995581f57c4297a1a09e5f3ba57f30","IPY_MODEL_f8688fc91fd7474aad3011d4c7d4745a"],"layout":"IPY_MODEL_6d31f05b5c3b47bf98e27063977ae2bf"}},"c874013e0c8d41178939086db4c901af":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e310a67f025e4097b51bbe9dc2c18241","placeholder":"​","style":"IPY_MODEL_c8d9bd26bd2a411784f5001da8f9bb30","value":"model.safetensors: 100%"}},"e9995581f57c4297a1a09e5f3ba57f30":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_387cc4b035cd4425b4c7b47b5d7aa163","max":345579424,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4afa1e04021d4b13beb3fbabbe5ae8a3","value":345579424}},"f8688fc91fd7474aad3011d4c7d4745a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ced200a5eca5457e9124749a452b1cb7","placeholder":"​","style":"IPY_MODEL_a84bb6c1fe134cc6ba81f180dd6039e9","value":" 346M/346M [00:01&lt;00:00, 328MB/s]"}},"6d31f05b5c3b47bf98e27063977ae2bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e310a67f025e4097b51bbe9dc2c18241":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8d9bd26bd2a411784f5001da8f9bb30":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"387cc4b035cd4425b4c7b47b5d7aa163":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4afa1e04021d4b13beb3fbabbe5ae8a3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ced200a5eca5457e9124749a452b1cb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a84bb6c1fe134cc6ba81f180dd6039e9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}