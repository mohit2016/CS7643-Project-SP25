{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "LYSEhsPs0Ulr",
        "outputId": "e0285043-1aa0-41a9-a1df-520a1e2c86e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in ./.venv/lib/python3.11/site-packages (25.0.1)\n",
            "Requirement already satisfied: tokenizers in ./.venv/lib/python3.11/site-packages (0.21.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./.venv/lib/python3.11/site-packages (from tokenizers) (0.30.2)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.12.0)\n",
            "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
            "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.1.31)\n",
            "Requirement already satisfied: datasets in ./.venv/lib/python3.11/site-packages (3.5.0)\n",
            "Requirement already satisfied: evaluate in ./.venv/lib/python3.11/site-packages (0.4.3)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.11/site-packages (from datasets) (2.2.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.11/site-packages (from datasets) (19.0.1)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in ./.venv/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in ./.venv/lib/python3.11/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in ./.venv/lib/python3.11/site-packages (from datasets) (3.11.16)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in ./.venv/lib/python3.11/site-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: transformers in ./.venv/lib/python3.11/site-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.venv/lib/python3.11/site-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.11/site-packages (from transformers) (2.2.4)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (2.2.4)\n",
            "Requirement already satisfied: torch in ./.venv/lib/python3.11/site-packages (2.6.0)\n",
            "Requirement already satisfied: matplotlib in ./.venv/lib/python3.11/site-packages (3.10.1)\n",
            "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (2.2.3)\n",
            "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.11/site-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (4.67.1)\n",
            "Requirement already satisfied: pillow in ./.venv/lib/python3.11/site-packages (11.2.1)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.11/site-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from torch) (2024.12.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.11/site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.11/site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.11/site-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: datasets in ./.venv/lib/python3.11/site-packages (3.5.0)\n",
            "Requirement already satisfied: evaluate in ./.venv/lib/python3.11/site-packages (0.4.3)\n",
            "Requirement already satisfied: transformers in ./.venv/lib/python3.11/site-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.11/site-packages (from datasets) (2.2.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.11/site-packages (from datasets) (19.0.1)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in ./.venv/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in ./.venv/lib/python3.11/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in ./.venv/lib/python3.11/site-packages (from datasets) (3.11.16)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in ./.venv/lib/python3.11/site-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: torchvision in ./.venv/lib/python3.11/site-packages (0.21.0)\n",
            "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from torchvision) (2.2.4)\n",
            "Requirement already satisfied: torch==2.6.0 in ./.venv/lib/python3.11/site-packages (from torchvision) (2.6.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.11/site-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (4.13.2)\n",
            "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (2024.12.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
            "Requirement already satisfied: setuptools in ./.venv/lib/python3.11/site-packages (78.1.0)\n",
            "Requirement already satisfied: wandb in ./.venv/lib/python3.11/site-packages (0.19.9)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in ./.venv/lib/python3.11/site-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in ./.venv/lib/python3.11/site-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in ./.venv/lib/python3.11/site-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in ./.venv/lib/python3.11/site-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in ./.venv/lib/python3.11/site-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in ./.venv/lib/python3.11/site-packages (from wandb) (7.0.0)\n",
            "Requirement already satisfied: pydantic<3 in ./.venv/lib/python3.11/site-packages (from wandb) (2.11.3)\n",
            "Requirement already satisfied: pyyaml in ./.venv/lib/python3.11/site-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in ./.venv/lib/python3.11/site-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in ./.venv/lib/python3.11/site-packages (from wandb) (2.26.1)\n",
            "Requirement already satisfied: setproctitle in ./.venv/lib/python3.11/site-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in ./.venv/lib/python3.11/site-packages (from wandb) (78.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in ./.venv/lib/python3.11/site-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: six>=1.4.0 in ./.venv/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in ./.venv/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.11/site-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in ./.venv/lib/python3.11/site-packages (from pydantic<3->wandb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in ./.venv/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Name: wandb\n",
            "Version: 0.19.9\n",
            "Summary: A CLI and library for interacting with the Weights & Biases API.\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: Weights & Biases <support@wandb.com>\n",
            "License: MIT License\n",
            "\n",
            "Copyright (c) 2021 Weights and Biases, Inc.\n",
            "\n",
            "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
            "of this software and associated documentation files (the \"Software\"), to deal\n",
            "in the Software without restriction, including without limitation the rights\n",
            "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
            "copies of the Software, and to permit persons to whom the Software is\n",
            "furnished to do so, subject to the following conditions:\n",
            "\n",
            "The above copyright notice and this permission notice shall be included in all\n",
            "copies or substantial portions of the Software.\n",
            "\n",
            "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
            "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
            "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
            "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
            "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
            "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
            "SOFTWARE.\n",
            "Location: /Users/egoh02/Github/CS7643-Project-SP25/.venv/lib/python3.11/site-packages\n",
            "Requires: click, docker-pycreds, gitpython, platformdirs, protobuf, psutil, pydantic, pyyaml, requests, sentry-sdk, setproctitle, setuptools, typing-extensions\n",
            "Required-by: \n",
            "Requirement already satisfied: schedulefree in ./.venv/lib/python3.11/site-packages (1.4.1)\n",
            "Requirement already satisfied: torch in ./.venv/lib/python3.11/site-packages (from schedulefree) (2.6.0)\n",
            "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.11/site-packages (from schedulefree) (4.13.2)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch->schedulefree) (3.18.0)\n",
            "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch->schedulefree) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch->schedulefree) (3.1.6)\n",
            "Requirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from torch->schedulefree) (2024.12.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.11/site-packages (from torch->schedulefree) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy==1.13.1->torch->schedulefree) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch->schedulefree) (3.0.2)\n",
            "Requirement already satisfied: nbformat in ./.venv/lib/python3.11/site-packages (5.10.4)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in ./.venv/lib/python3.11/site-packages (from nbformat) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in ./.venv/lib/python3.11/site-packages (from nbformat) (4.23.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./.venv/lib/python3.11/site-packages (from nbformat) (5.7.2)\n",
            "Requirement already satisfied: traitlets>=5.1 in ./.venv/lib/python3.11/site-packages (from nbformat) (5.14.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in ./.venv/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat) (0.24.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in ./.venv/lib/python3.11/site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.3.7)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in ./.venv/lib/python3.11/site-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat) (4.13.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install tokenizers\n",
        "!pip install datasets --upgrade evaluate\n",
        "!pip install transformers\n",
        "!pip install numpy torch matplotlib pandas scikit-learn tqdm pillow\n",
        "!pip install datasets evaluate transformers\n",
        "!pip install torchvision\n",
        "!pip install setuptools\n",
        "!pip install wandb\n",
        "!pip show wandb\n",
        "!pip install schedulefree\n",
        "!pip install nbformat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nW76RvsBd0lZ",
        "outputId": "b8baebfa-b9de-4efd-e82b-fb778af23954"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from evaluate import load\n",
        "from transformers import (\n",
        "    ViTFeatureExtractor,\n",
        "    ViTForImageClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    get_scheduler,\n",
        "    AutoImageProcessor\n",
        ")\n",
        "\n",
        "from torch.optim import AdamW, SGD\n",
        "import wandb\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from schedulefree import AdamWScheduleFree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "hsPdHp-P0Ult"
      },
      "outputs": [],
      "source": [
        "# sweep_config = {\n",
        "#     \"method\": \"grid\",  # we cna use \"grid\", \"random\", or \"bayes\"\n",
        "#     \"metric\": {\n",
        "#         \"name\": \"val_accuracy\",  # Metric to optimize\n",
        "#         \"goal\": \"maximize\"       # Goal: maximize or minimize\n",
        "#     },\n",
        "#     \"parameters\": {\n",
        "#         \"optimizer_name\": {\n",
        "#             \"values\": [\"AdamW\", \"SGD\", \"RMSProp\", \"AdaGrad\", \"schedule_free_adamw\"]  # Optimizers to test\n",
        "#         },\n",
        "#         \"learning_rate\": {\n",
        "#             \"values\": [2e-5, 2e-4, 2e-3, 2e-2, 2e-1]  # Fixed learning rate for simplicity\n",
        "#         },\n",
        "#         \"batch_size\": {\n",
        "#             \"values\": [16]  # Fixed batch size\n",
        "#         },\n",
        "#         \"num_epochs\": {\n",
        "#             \"values\": [3]  # Fixed number of epochs\n",
        "#         },\n",
        "#         \"scheduler_name\": {\n",
        "#             \"values\": [\"cosine\"]  # Fixed scheduler for simplicity\n",
        "#         }\n",
        "#     }\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "M5m2CPNN0Ulu"
      },
      "outputs": [],
      "source": [
        "# Set seed for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed()\n",
        "\n",
        "# Initialize experiment tracking\n",
        "def init_wandb(project_name, experiment_name, config):\n",
        "    return wandb.init(\n",
        "        # entity=\"dl_project_sp25\",\n",
        "        project=project_name,\n",
        "        name=experiment_name,\n",
        "        config=config,\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "# Load and prepare dataset\n",
        "def prepare_dataset(dataset_name, image_processor):\n",
        "    \"\"\"\n",
        "    Load and prepare a dataset from Hugging Face for ViT fine-tuning\n",
        "    \"\"\"\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset: {dataset_name}\")\n",
        "    dataset = load_dataset(dataset_name)\n",
        "\n",
        "    # Get label information\n",
        "    if \"label\" in dataset[\"train\"].features:\n",
        "        labels = dataset[\"train\"].features[\"label\"].names\n",
        "    elif \"labels\" in dataset[\"train\"].features:\n",
        "        labels = dataset[\"train\"].features[\"labels\"].names\n",
        "    else:\n",
        "        # Count unique labels and create labels list\n",
        "        all_labels = dataset[\"train\"][0][\"label\"] if \"label\" in dataset[\"train\"][0] else dataset[\"train\"][0][\"labels\"]\n",
        "        num_labels = len(set(all_labels))\n",
        "        labels = [str(i) for i in range(num_labels)]\n",
        "\n",
        "    # Create label mappings\n",
        "    label2id = {label: i for i, label in enumerate(labels)}\n",
        "    id2label = {i: label for i, label in enumerate(labels)}\n",
        "\n",
        "    # Set up image transformations based on the model's requirements\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=image_processor.image_mean,\n",
        "        std=image_processor.image_std\n",
        "    )\n",
        "\n",
        "    # Get the expected image size\n",
        "    if \"shortest_edge\" in image_processor.size:\n",
        "        size = image_processor.size[\"shortest_edge\"]\n",
        "    else:\n",
        "        size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
        "\n",
        "    # Define transforms for training data\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "\n",
        "    # Define transforms for validation/test data\n",
        "    val_transforms = transforms.Compose([\n",
        "        transforms.Resize(size),\n",
        "        transforms.CenterCrop(size),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "\n",
        "    # Apply transformations to the dataset\n",
        "    def preprocess_train(examples):\n",
        "        examples[\"pixel_values\"] = [\n",
        "            train_transforms(image.convert(\"RGB\"))\n",
        "            for image in examples[\"image\"]\n",
        "        ]\n",
        "        return examples\n",
        "\n",
        "    def preprocess_val(examples):\n",
        "        examples[\"pixel_values\"] = [\n",
        "            val_transforms(image.convert(\"RGB\"))\n",
        "            for image in examples[\"image\"]\n",
        "        ]\n",
        "        return examples\n",
        "\n",
        "    # Apply preprocessing to each split\n",
        "    train_dataset = dataset[\"train\"].map(\n",
        "        preprocess_train, batched=True, remove_columns=[\"image\"]\n",
        "    )\n",
        "\n",
        "    if \"validation\" in dataset:\n",
        "        val_dataset = dataset[\"validation\"].map(\n",
        "            preprocess_val, batched=True, remove_columns=[\"image\"]\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        # Create a validation split if none exists\n",
        "        splits = train_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "        train_dataset = splits[\"train\"]\n",
        "        val_dataset = splits[\"test\"]\n",
        "\n",
        "    if \"test\" in dataset:\n",
        "        test_dataset = dataset[\"test\"].map(\n",
        "            preprocess_val, batched=True, remove_columns=[\"image\"]\n",
        "        )\n",
        "    else:\n",
        "        # test_dataset = val_dataset    #split further rather than using validation as test dataset\n",
        "\n",
        "        # Further split validation dataset to create a test dataset\n",
        "        test_split = val_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "        val_dataset = test_split[\"train\"]  # Update validation dataset\n",
        "        test_dataset = test_split[\"test\"]  # Create test dataset\n",
        "\n",
        "    print(f\"Dataset prepared with {len(train_dataset)} training, {len(val_dataset)} validation, and {len(test_dataset)} test examples\")\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset, id2label, label2id\n",
        "\n",
        "# Define compute_metrics function for evaluation\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "    }\n",
        "\n",
        "# Main experiment pipeline\n",
        "def run_vit_experiment(config):\n",
        "    \"\"\"\n",
        "    Run a ViT experiment with the specified configuration\n",
        "    \"\"\"\n",
        "    # Initialize wandb for experiment tracking\n",
        "    run = init_wandb(\"ViT-LR-Schedulers\", config[\"experiment_name\"], config)\n",
        "\n",
        "    # Load the image processor for the model\n",
        "    image_processor = AutoImageProcessor.from_pretrained(config[\"model_name\"], use_fast=True)\n",
        "\n",
        "    # Prepare the dataset\n",
        "    train_dataset, val_dataset, test_dataset, id2label, label2id = prepare_dataset(\n",
        "        config[\"dataset_name\"], image_processor\n",
        "    )\n",
        "\n",
        "    # # Visualize some images from the training dataset (do this w/o the remove_columns=[\"image\"])\n",
        "    # # Initialize a set to keep track of shown labels\n",
        "    # shown_labels = set()\n",
        "\n",
        "    # # Initialize the figure for plotting\n",
        "    # plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # # Loop through the dataset and plot the first image of each label\n",
        "    # for i, sample in enumerate(train_dataset):\n",
        "    #     label = sample[\"label\"]\n",
        "    #     image = sample[\"image\"]\n",
        "\n",
        "    #     # Check if the label has already been shown\n",
        "    #     if label not in shown_labels:\n",
        "    #         plt.subplot(1, len(id2label), len(shown_labels) + 1)\n",
        "    #         plt.imshow(image.convert(\"RGB\"))  # Convert to RGB if necessary\n",
        "    #         plt.title(id2label[label])  # Get label name\n",
        "    #         plt.axis(\"off\")\n",
        "    #         shown_labels.add(label)\n",
        "\n",
        "    #         # Stop if all labels have been shown\n",
        "    #         if len(shown_labels) == len(id2label):\n",
        "    #             break\n",
        "\n",
        "    # plt.show()\n",
        "\n",
        "    # Load the ViT model\n",
        "    model = ViTForImageClassification.from_pretrained(\n",
        "        config[\"model_name\"],\n",
        "        num_labels=len(id2label),\n",
        "        id2label=id2label,\n",
        "        label2id=label2id,\n",
        "        ignore_mismatched_sizes=True\n",
        "    )\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results/{config['experiment_name']}\",\n",
        "        per_device_train_batch_size=config[\"batch_size\"],\n",
        "        per_device_eval_batch_size=config[\"batch_size\"],\n",
        "        num_train_epochs=config[\"num_epochs\"],\n",
        "        weight_decay=config[\"weight_decay\"],\n",
        "        eval_strategy=\"steps\",\n",
        "        save_strategy=\"steps\",\n",
        "        logging_strategy=\"steps\",  # Ensure logging is enabled\n",
        "        logging_steps=10,          # Log every 10 steps (adjust as needed)\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        push_to_hub=False,\n",
        "        report_to=\"wandb\",\n",
        "        remove_unused_columns=False,\n",
        "        learning_rate=config[\"learning_rate\"],\n",
        "    )\n",
        "\n",
        "    # Setup optimizer\n",
        "    if config[\"optimizer_name\"] == \"AdamW\":\n",
        "        optimizer = AdamW(model.parameters(), lr=config[\"learning_rate\"])\n",
        "    else:  # SGD\n",
        "        optimizer = SGD(model.parameters(), lr=config[\"learning_rate\"], momentum=0.9)\n",
        "\n",
        "    # Setup scheduler\n",
        "    num_training_steps = len(train_dataset) // config[\"batch_size\"] * config[\"num_epochs\"]\n",
        "    num_warmup_steps = int(num_training_steps * config[\"warmup_ratio\"]) if \"warmup_ratio\" in config else 0\n",
        "\n",
        "    scheduler_name = config[\"scheduler_name\"]\n",
        "    if scheduler_name == \"linear\":\n",
        "        scheduler = get_scheduler(\n",
        "            \"linear\",\n",
        "            optimizer=optimizer,\n",
        "            num_warmup_steps=num_warmup_steps,\n",
        "            num_training_steps=num_training_steps\n",
        "        )\n",
        "    elif scheduler_name == \"cosine\":\n",
        "        scheduler = get_scheduler(\n",
        "            \"cosine\",\n",
        "            optimizer=optimizer,\n",
        "            num_warmup_steps=num_warmup_steps,\n",
        "            num_training_steps=num_training_steps\n",
        "        )\n",
        "    elif scheduler_name == \"cosine_with_restarts\":\n",
        "        scheduler = get_scheduler(\n",
        "            \"cosine_with_restarts\",\n",
        "            optimizer=optimizer,\n",
        "            num_warmup_steps=num_warmup_steps,\n",
        "            num_training_steps=num_training_steps,\n",
        "        )\n",
        "    elif scheduler_name == \"polynomial\":\n",
        "        scheduler = get_scheduler(\n",
        "            \"polynomial\",\n",
        "            optimizer=optimizer,\n",
        "            num_warmup_steps=num_warmup_steps,\n",
        "            num_training_steps=num_training_steps,\n",
        "            # power=config.get(\"poly_power\", 1.0),\n",
        "        )\n",
        "    elif scheduler_name == \"constant\":\n",
        "        scheduler = get_scheduler(\n",
        "            \"constant\",\n",
        "            optimizer=optimizer,\n",
        "        )\n",
        "    elif scheduler_name == \"constant_with_warmup\":\n",
        "        scheduler = get_scheduler(\n",
        "            \"constant_with_warmup\",\n",
        "            optimizer=optimizer,\n",
        "            num_warmup_steps=num_warmup_steps,\n",
        "        )\n",
        "    # add more experiments.\n",
        "\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Scheduler {scheduler_name} not supported\")\n",
        "\n",
        "    # Initialize Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        optimizers=(optimizer, scheduler)\n",
        "    )\n",
        "\n",
        "    # # Loss validation curve in the training loop to log metrics to W&B\n",
        "    # for epoch in range(config[\"num_epochs\"]):\n",
        "    #     print(f\"Epoch {epoch + 1}/{config['num_epochs']}\")\n",
        "\n",
        "    #     # Train for one epoch\n",
        "    #     trainer.train()\n",
        "\n",
        "    #     # Evaluate on validation set\n",
        "    #     eval_results = trainer.evaluate(val_dataset)\n",
        "\n",
        "    #     print(trainer.state.log_history)\n",
        "    #     if trainer.state.log_history and \"loss\" in trainer.state.log_history[-1]:\n",
        "    #         train_loss = trainer.state.log_history[-1][\"loss\"]\n",
        "    #     else:\n",
        "    #         train_loss = None\n",
        "\n",
        "    #     # Log training and validation metrics to W&B\n",
        "    #     wandb.log({\n",
        "    #         \"epoch\": epoch + 1,\n",
        "    #         \"train_loss\": trainer.state.log_history[-1].get(\"loss\", None),\n",
        "    #         \"val_loss\": eval_results[\"eval_loss\"],\n",
        "    #         \"val_accuracy\": eval_results[\"eval_accuracy\"],\n",
        "    #     })\n",
        "\n",
        "    # # Loss epoch curve in the training loop to log metrics to W&B\n",
        "    # for epoch in range(config[\"num_epochs\"]):\n",
        "    #     print(f\"Epoch {epoch + 1}/{config['num_epochs']}\")\n",
        "\n",
        "    #     # Train for one epoch\n",
        "    #     trainer.train()\n",
        "\n",
        "    #     # Evaluate on validation set\n",
        "    #     eval_results = trainer.evaluate(val_dataset)\n",
        "\n",
        "    #     # Extract training loss from the trainer's state\n",
        "    #     if trainer.state.log_history and \"loss\" in trainer.state.log_history[-1]:\n",
        "    #         train_loss = trainer.state.log_history[-1][\"loss\"]\n",
        "    #     else:\n",
        "    #         train_loss = None  # Handle missing loss gracefully\n",
        "\n",
        "    #     # Log training and validation metrics to W&B\n",
        "    #     wandb.log({\n",
        "    #         \"epoch\": epoch + 1,\n",
        "    #         \"train_loss\": train_loss,                  # Training loss\n",
        "    #         \"val_loss\": eval_results[\"eval_loss\"],    # Validation loss\n",
        "    #         \"val_accuracy\": eval_results[\"eval_accuracy\"],  # Validation accuracy\n",
        "    #     })\n",
        "\n",
        "    # Train the model\n",
        "    print(f\"Starting training for {config['experiment_name']}...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(f\"Evaluating {config['experiment_name']}...\")\n",
        "    eval_results = trainer.evaluate(test_dataset)\n",
        "\n",
        "\n",
        "    # Log final metrics\n",
        "    wandb.log({\n",
        "        \"final_accuracy\": eval_results[\"eval_accuracy\"],\n",
        "        \"final_f1\": eval_results[\"eval_f1\"],\n",
        "        \"final_precision\": eval_results[\"eval_precision\"],\n",
        "        \"final_recall\": eval_results[\"eval_recall\"],\n",
        "    })\n",
        "\n",
        "    # Compute confusion matrix for test set\n",
        "    predictions, labels, _ = trainer.predict(test_dataset)\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Convert to lists\n",
        "    labels = labels.tolist()\n",
        "    predictions = predictions.tolist()\n",
        "\n",
        "    # Log confusion matrix to W&B\n",
        "    wandb.log({\n",
        "        \"confusion_matrix_test\": wandb.plot.confusion_matrix(\n",
        "            probs=None,\n",
        "            y_true=labels,\n",
        "            preds=predictions,\n",
        "            class_names=[str(i) for i in range(len(np.unique(labels)))]\n",
        "        )\n",
        "    })\n",
        "\n",
        "\n",
        "\n",
        "    # Save the model\n",
        "    trainer.save_model(f\"./saved_models/{config['experiment_name']}\")\n",
        "\n",
        "    # Finish wandb run\n",
        "    wandb.finish()\n",
        "\n",
        "    return eval_results\n",
        "\n",
        "# Get experiment configurations for challenging datasets\n",
        "def get_experiment_configs():\n",
        "    # We'll use a more complex dataset from Hugging Face\n",
        "    base_config = {\n",
        "        \"model_name\": \"google/vit-base-patch16-224-in21k\",\n",
        "        \"dataset_name\": \"jbarat/plant_species\",  # Any challenging dataset.\n",
        "        \"batch_size\": 16,\n",
        "        \"num_epochs\": 3, # let's keep smaller number to begin with.\n",
        "        \"weight_decay\": 0.01,\n",
        "        \"optimizer_name\": \"AdamW\",\n",
        "    }\n",
        "\n",
        "    # Different learning rate scheduler configurations\n",
        "    configs = []\n",
        "\n",
        "    # Constant learning rate (baseline)\n",
        "    configs.append({\n",
        "        **base_config,\n",
        "        \"experiment_name\": \"vit_constant_lr\",\n",
        "        \"learning_rate\": 5e-5,\n",
        "        \"scheduler_name\": \"constant\",\n",
        "    })\n",
        "\n",
        "    # Linear decay\n",
        "    configs.append({\n",
        "        **base_config,\n",
        "        \"experiment_name\": \"vit_linear_decay\",\n",
        "        \"learning_rate\": 5e-5,\n",
        "        \"scheduler_name\": \"linear\",\n",
        "        \"warmup_ratio\": 0.1,\n",
        "    })\n",
        "\n",
        "    # Cosine decay (commonly used with ViT)\n",
        "    configs.append({\n",
        "        **base_config,\n",
        "        \"experiment_name\": \"vit_cosine_decay\",\n",
        "        \"learning_rate\": 5e-5,\n",
        "        \"scheduler_name\": \"cosine\",\n",
        "        \"warmup_ratio\": 0.1,\n",
        "    })\n",
        "\n",
        "    # Cosine with restarts\n",
        "    configs.append({\n",
        "        **base_config,\n",
        "        \"experiment_name\": \"vit_cosine_restarts\",\n",
        "        \"learning_rate\": 5e-5,\n",
        "        \"scheduler_name\": \"cosine_with_restarts\",\n",
        "        \"warmup_ratio\": 0.1,\n",
        "    })\n",
        "\n",
        "    # Polynomial decay\n",
        "    configs.append({\n",
        "        **base_config,\n",
        "        \"experiment_name\": \"vit_polynomial\",\n",
        "        \"learning_rate\": 5e-5,\n",
        "        \"scheduler_name\": \"polynomial\",\n",
        "        \"warmup_ratio\": 0.1,\n",
        "        \"poly_power\": 2.0,\n",
        "    })\n",
        "\n",
        "    # Constant with warmup\n",
        "    configs.append({\n",
        "        **base_config,\n",
        "        \"experiment_name\": \"vit_constant_warmup\",\n",
        "        \"learning_rate\": 5e-5,\n",
        "        \"scheduler_name\": \"constant_with_warmup\",\n",
        "        \"warmup_ratio\": 0.1,\n",
        "    })\n",
        "\n",
        "    # Different learning rate experiments\n",
        "    for lr in [1e-5, 3e-5, 1e-4]:\n",
        "        configs.append({\n",
        "            **base_config,\n",
        "            \"experiment_name\": f\"vit_cosine_lr_{lr}\",\n",
        "            \"learning_rate\": lr,\n",
        "            \"scheduler_name\": \"cosine\",\n",
        "            \"warmup_ratio\": 0.1,\n",
        "        })\n",
        "\n",
        "    # Different optimizer experiments\n",
        "    configs.append({\n",
        "        **base_config,\n",
        "        \"experiment_name\": \"vit_sgd_cosine\",\n",
        "        \"learning_rate\": 0.01,  # Higher LR for SGD\n",
        "        \"scheduler_name\": \"cosine\",\n",
        "        \"warmup_ratio\": 0.1,\n",
        "        \"optimizer_name\": \"SGD\",\n",
        "    })\n",
        "\n",
        "    # here we can make changes to add new datasets to experiment.\n",
        "    # or change batch_size to see the impact.\n",
        "    # Other datasets to try (uncomment to use)\n",
        "    #   Erik: We can use a data set as a strech. Maybe something less similar than plants for better contrasting comparison?\n",
        "    # flowers dataset\n",
        "    # configs.append({\n",
        "    #     **base_config,\n",
        "    #     \"dataset_name\": \"huggan/flowers\",\n",
        "    #     \"experiment_name\": \"vit_flowers_cosine\",\n",
        "    #     \"learning_rate\": 5e-5,\n",
        "    #     \"scheduler_name\": \"cosine\",\n",
        "    #     \"warmup_ratio\": 0.1,\n",
        "    # })\n",
        "\n",
        "    return configs\n",
        "\n",
        "# Run experiments and visualize results\n",
        "def run_all_experiments():\n",
        "    configs = get_experiment_configs()\n",
        "    results = []\n",
        "\n",
        "    for config in configs:\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Running experiment: {config['experiment_name']}\")\n",
        "        print(f\"{'='*50}\\n\")\n",
        "\n",
        "        eval_results = run_vit_experiment(config)\n",
        "        results.append({\n",
        "            \"experiment\": config['experiment_name'],\n",
        "            \"accuracy\": eval_results[\"eval_accuracy\"],\n",
        "            \"f1\": eval_results[\"eval_f1\"],\n",
        "            \"precision\": eval_results[\"eval_precision\"],\n",
        "            \"recall\": eval_results[\"eval_recall\"],\n",
        "            \"config\": config\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Visualize and compare results\n",
        "def visualize_results(results):\n",
        "    # Create DataFrame for easier plotting\n",
        "    df = pd.DataFrame([\n",
        "        {\n",
        "            \"Experiment\": result[\"experiment\"],\n",
        "            \"Accuracy\": result[\"accuracy\"],\n",
        "            \"F1 Score\": result[\"f1\"],\n",
        "            \"Precision\": result[\"precision\"],\n",
        "            \"Recall\": result[\"recall\"],\n",
        "            \"Learning Rate\": result[\"config\"][\"learning_rate\"],\n",
        "            \"Scheduler\": result[\"config\"][\"scheduler_name\"],\n",
        "            \"Optimizer\": result[\"config\"][\"optimizer_name\"],\n",
        "            \"Dataset\": result[\"config\"][\"dataset_name\"]\n",
        "        }\n",
        "        for result in results\n",
        "    ])\n",
        "\n",
        "    # Plot accuracy comparison\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    ax = plt.bar(df[\"Experiment\"], df[\"Accuracy\"], color='skyblue')\n",
        "    plt.xlabel('Experiment')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Comparison of Model Accuracy Across Experiments')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"accuracy_comparison.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Plot all metrics for a more comprehensive comparison\n",
        "    plt.figure(figsize=(16, 10))\n",
        "    metrics = [\"Accuracy\", \"F1 Score\", \"Precision\", \"Recall\"]\n",
        "    x = np.arange(len(df[\"Experiment\"]))\n",
        "    width = 0.2\n",
        "\n",
        "    for i, metric in enumerate(metrics):\n",
        "        plt.bar(x + i*width, df[metric], width=width, label=metric)\n",
        "\n",
        "    plt.xlabel('Experiment')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Comparison of Metrics Across Experiments')\n",
        "    plt.xticks(x + width*1.5, df[\"Experiment\"], rotation=45, ha='right')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"metrics_comparison.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Plot results by scheduler type\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    schedulers = df[\"Scheduler\"].unique()\n",
        "    for scheduler in schedulers:\n",
        "        scheduler_data = df[df[\"Scheduler\"] == scheduler]\n",
        "        plt.plot(scheduler_data[\"Learning Rate\"], scheduler_data[\"Accuracy\"], 'o-', label=scheduler)\n",
        "\n",
        "    plt.xlabel('Learning Rate')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy vs. Learning Rate by Scheduler Type')\n",
        "    plt.xscale('log')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"scheduler_comparison.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Create a table with results\n",
        "    print(\"Results Summary:\")\n",
        "    print(df[[\"Experiment\", \"Accuracy\", \"F1 Score\", \"Precision\", \"Recall\", \"Scheduler\", \"Learning Rate\", \"Optimizer\", \"Dataset\"]])\n",
        "\n",
        "    # Save results to CSV\n",
        "    df.to_csv(\"experiment_results.csv\", index=False)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Function to run a single experiment (useful for testing)\n",
        "def run_single_experiment(experiment_index=0):\n",
        "    configs = get_experiment_configs()\n",
        "    if experiment_index >= len(configs):\n",
        "        print(f\"Invalid experiment index. Choose between 0 and {len(configs)-1}\")\n",
        "        return\n",
        "\n",
        "    config = configs[experiment_index]\n",
        "    print(f\"Running single experiment: {config['experiment_name']}\")\n",
        "    eval_results = run_vit_experiment(config)\n",
        "\n",
        "    print(f\"\\nResults for {config['experiment_name']}:\")\n",
        "    print(f\"Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
        "    print(f\"F1 Score: {eval_results['eval_f1']:.4f}\")\n",
        "    print(f\"Precision: {eval_results['eval_precision']:.4f}\")\n",
        "    print(f\"Recall: {eval_results['eval_recall']:.4f}\")\n",
        "\n",
        "    return eval_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_optimizer_sweep():\n",
        "    # Initialize W&B run first, then access config\n",
        "    with wandb.init() as run:\n",
        "        print(f\"W&B initialized: {run.name}\")\n",
        "        \n",
        "        # Get config from sweep\n",
        "        config = wandb.config\n",
        "        \n",
        "        # Set experiment name based on sweep parameters\n",
        "        custom_name = f\"vit_{config.optimizer_name}_{config.learning_rate}\"\n",
        "        # Update the run name after initialization\n",
        "        wandb.run.name = custom_name\n",
        "        wandb.run.save()\n",
        "        \n",
        "        print(f\"Running experiment: {custom_name}\")\n",
        "        \n",
        "        # Load model and processor\n",
        "        model_name = \"google/vit-base-patch16-224-in21k\"\n",
        "        dataset_name = \"jbarat/plant_species\"\n",
        "        \n",
        "        # Load the image processor\n",
        "        image_processor = AutoImageProcessor.from_pretrained(model_name, use_fast=True)\n",
        "        \n",
        "        # Prepare dataset\n",
        "        train_dataset, val_dataset, test_dataset, id2label, label2id = prepare_dataset(\n",
        "            dataset_name, image_processor\n",
        "        )\n",
        "        \n",
        "        # Load the ViT model\n",
        "        model = ViTForImageClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=len(id2label),\n",
        "            id2label=id2label,\n",
        "            label2id=label2id,\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "        \n",
        "        # Define training arguments\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=f\"./results/{custom_name}\",\n",
        "            per_device_train_batch_size=config.batch_size,\n",
        "            per_device_eval_batch_size=config.batch_size,\n",
        "            num_train_epochs=config.num_epochs,\n",
        "            weight_decay=0.01,\n",
        "            eval_strategy=\"steps\",\n",
        "            save_strategy=\"steps\",\n",
        "            logging_strategy=\"steps\",\n",
        "            logging_steps=10,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"accuracy\",\n",
        "            push_to_hub=False,\n",
        "            report_to=\"wandb\",\n",
        "            remove_unused_columns=False,\n",
        "            learning_rate=config.learning_rate,\n",
        "        )\n",
        "        # Set up optimizer based on config\n",
        "        if config.optimizer_name == \"schedule_free_adamw\":\n",
        "            optimizer = AdamWScheduleFree(\n",
        "                model.parameters(),\n",
        "                lr=config.learning_rate,  # Learning rate\n",
        "                # warmup_steps=500  # Optional: Adjust based on your dataset\n",
        "            )\n",
        "        elif config.optimizer_name == \"AdamW\":\n",
        "            optimizer = AdamW(model.parameters(), lr=config.learning_rate)\n",
        "        elif config.optimizer_name == \"SGD\":\n",
        "            optimizer = SGD(model.parameters(), lr=config.learning_rate, momentum=0.9)\n",
        "        elif config.optimizer_name == \"RMSProp\":\n",
        "            optimizer = torch.optim.RMSprop(model.parameters(), lr=config.learning_rate)\n",
        "        elif config.optimizer_name == \"AdaGrad\":\n",
        "            optimizer = torch.optim.Adagrad(model.parameters(), lr=config.learning_rate)\n",
        "        else:\n",
        "            optimizer = AdamW(model.parameters(), lr=config.learning_rate)        \n",
        "        \n",
        "        # Setup scheduler\n",
        "        num_training_steps = len(train_dataset) // config.batch_size * config.num_epochs\n",
        "        num_warmup_steps = int(num_training_steps * 0.1)  # 10% warmup\n",
        "        \n",
        "        scheduler = get_scheduler(\n",
        "            config.scheduler_name,\n",
        "            optimizer=optimizer,\n",
        "            num_warmup_steps=num_warmup_steps,\n",
        "            num_training_steps=num_training_steps\n",
        "        )\n",
        "        \n",
        "        # Initialize Trainer\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            compute_metrics=compute_metrics,\n",
        "            optimizers=(optimizer, scheduler)\n",
        "        )\n",
        "        \n",
        "        # Train the model\n",
        "        print(f\"Starting training...\")\n",
        "        # optimizer.train()  # Switch optimizer to training mode only for schedule_free\n",
        "        trainer.train()\n",
        "        \n",
        "        # Evaluate on validation dataset\n",
        "        print(f\"Evaluating on validation set...\")\n",
        "        # optimizer.eval()  # Switch optimizer to evaluation mode only for schedule_free\n",
        "        eval_results = trainer.evaluate(val_dataset)\n",
        "        \n",
        "        # Log validation metrics\n",
        "        run.log({\n",
        "            \"val_accuracy\": eval_results[\"eval_accuracy\"],\n",
        "            \"val_f1\": eval_results[\"eval_f1\"],\n",
        "            \"val_precision\": eval_results[\"eval_precision\"],\n",
        "            \"val_recall\": eval_results[\"eval_recall\"],\n",
        "            \"val_loss\": eval_results[\"eval_loss\"]\n",
        "        })\n",
        "        \n",
        "        # Evaluate on test dataset\n",
        "        print(f\"Evaluating on test set...\")\n",
        "        test_results = trainer.evaluate(test_dataset)\n",
        "        \n",
        "        # Log test metrics\n",
        "        run.log({\n",
        "            \"test_accuracy\": test_results[\"eval_accuracy\"],\n",
        "            \"test_f1\": test_results[\"eval_f1\"],\n",
        "            \"test_precision\": test_results[\"eval_precision\"],\n",
        "            \"test_recall\": test_results[\"eval_recall\"],\n",
        "            \"test_loss\": test_results[\"eval_loss\"]\n",
        "        })\n",
        "        \n",
        "        # Compute confusion matrix for test set\n",
        "        predictions, labels, _ = trainer.predict(test_dataset)\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "        \n",
        "        # Log confusion matrix\n",
        "        run.log({\n",
        "            \"confusion_matrix\": wandb.plot.confusion_matrix(\n",
        "                probs=None,\n",
        "                y_true=labels.tolist(),\n",
        "                preds=predictions.tolist(),\n",
        "                class_names=[id2label[i] for i in range(len(id2label))]\n",
        "            )\n",
        "        })\n",
        "        \n",
        "        # Save the model\n",
        "        model_path = f\"./saved_models/{custom_name}\"\n",
        "        trainer.save_model(model_path)\n",
        "        print(f\"Model saved to {model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AgLdeTtFtIgc",
        "outputId": "457c59b4-f96e-4fbd-cd24-e66e95d759c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting ViT experiments with different learning rate schedulers...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create sweep with ID: 8v6px2kf\n",
            "Sweep URL: https://wandb.ai/dl_project_sp25/ViT-Optimizer-Sweep/sweeps/8v6px2kf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 06jzobdu with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0002\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_name: AdamW\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: cosine\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/egoh02/Github/CS7643-Project-SP25/wandb/run-20250419_224419-06jzobdu</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dl_project_sp25/ViT-Optimizer-Sweep/runs/06jzobdu' target=\"_blank\">treasured-sweep-1</a></strong> to <a href='https://wandb.ai/dl_project_sp25/ViT-Optimizer-Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/dl_project_sp25/ViT-Optimizer-Sweep/sweeps/8v6px2kf' target=\"_blank\">https://wandb.ai/dl_project_sp25/ViT-Optimizer-Sweep/sweeps/8v6px2kf</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/dl_project_sp25/ViT-Optimizer-Sweep' target=\"_blank\">https://wandb.ai/dl_project_sp25/ViT-Optimizer-Sweep</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/dl_project_sp25/ViT-Optimizer-Sweep/sweeps/8v6px2kf' target=\"_blank\">https://wandb.ai/dl_project_sp25/ViT-Optimizer-Sweep/sweeps/8v6px2kf</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/dl_project_sp25/ViT-Optimizer-Sweep/runs/06jzobdu' target=\"_blank\">https://wandb.ai/dl_project_sp25/ViT-Optimizer-Sweep/runs/06jzobdu</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W&B initialized: treasured-sweep-1\n",
            "Running experiment: vit_AdamW_0.0002\n",
            "Loading dataset: jbarat/plant_species\n",
            "Dataset prepared with 640 training, 128 validation, and 32 test examples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [120/120 04:28, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.012000</td>\n",
              "      <td>1.826977</td>\n",
              "      <td>0.585938</td>\n",
              "      <td>0.544083</td>\n",
              "      <td>0.542073</td>\n",
              "      <td>0.585938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.642400</td>\n",
              "      <td>1.390716</td>\n",
              "      <td>0.726562</td>\n",
              "      <td>0.716399</td>\n",
              "      <td>0.768291</td>\n",
              "      <td>0.726562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.342700</td>\n",
              "      <td>1.110769</td>\n",
              "      <td>0.789062</td>\n",
              "      <td>0.784859</td>\n",
              "      <td>0.805987</td>\n",
              "      <td>0.789062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.038500</td>\n",
              "      <td>0.947948</td>\n",
              "      <td>0.757812</td>\n",
              "      <td>0.760557</td>\n",
              "      <td>0.775360</td>\n",
              "      <td>0.757812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.688100</td>\n",
              "      <td>0.888995</td>\n",
              "      <td>0.773438</td>\n",
              "      <td>0.770746</td>\n",
              "      <td>0.791871</td>\n",
              "      <td>0.773438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.664800</td>\n",
              "      <td>0.726733</td>\n",
              "      <td>0.820312</td>\n",
              "      <td>0.816785</td>\n",
              "      <td>0.827519</td>\n",
              "      <td>0.820312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.528200</td>\n",
              "      <td>0.795668</td>\n",
              "      <td>0.742188</td>\n",
              "      <td>0.745829</td>\n",
              "      <td>0.778188</td>\n",
              "      <td>0.742188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.545100</td>\n",
              "      <td>0.610374</td>\n",
              "      <td>0.828125</td>\n",
              "      <td>0.823289</td>\n",
              "      <td>0.829726</td>\n",
              "      <td>0.828125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.358500</td>\n",
              "      <td>0.593596</td>\n",
              "      <td>0.828125</td>\n",
              "      <td>0.829610</td>\n",
              "      <td>0.838593</td>\n",
              "      <td>0.828125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.316700</td>\n",
              "      <td>0.603395</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.812537</td>\n",
              "      <td>0.817605</td>\n",
              "      <td>0.812500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.312100</td>\n",
              "      <td>0.591100</td>\n",
              "      <td>0.835938</td>\n",
              "      <td>0.836147</td>\n",
              "      <td>0.848735</td>\n",
              "      <td>0.835938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.290300</td>\n",
              "      <td>0.588932</td>\n",
              "      <td>0.835938</td>\n",
              "      <td>0.836147</td>\n",
              "      <td>0.848735</td>\n",
              "      <td>0.835938</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on validation set...\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on test set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/egoh02/Github/CS7643-Project-SP25/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/Users/egoh02/Github/CS7643-Project-SP25/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to ./saved_models/vit_AdamW_0.0002\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▅▇▆▆█▅██▇███▅</td></tr><tr><td>eval/f1</td><td>▁▅▇▆▆█▆██▇███▄</td></tr><tr><td>eval/loss</td><td>█▆▄▃▃▂▂▁▁▁▁▁▁▂</td></tr><tr><td>eval/precision</td><td>▁▆▇▆▇█▆██▇███▃</td></tr><tr><td>eval/recall</td><td>▁▅▇▆▆█▅██▇███▅</td></tr><tr><td>eval/runtime</td><td>█▆▇▇▇▇▇▇▇▇▇▇█▁</td></tr><tr><td>eval/samples_per_second</td><td>▂█▇▆▅▄▄▇▆▃▇▇▁▄</td></tr><tr><td>eval/steps_per_second</td><td>▂█▇▆▅▄▄▇▆▃▇▇▁▄</td></tr><tr><td>test/accuracy</td><td>▁</td></tr><tr><td>test/f1</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>test/precision</td><td>▁</td></tr><tr><td>test/recall</td><td>▁</td></tr><tr><td>test/runtime</td><td>▁</td></tr><tr><td>test/samples_per_second</td><td>▁</td></tr><tr><td>test/steps_per_second</td><td>▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>test_precision</td><td>▁</td></tr><tr><td>test_recall</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇█████████</td></tr><tr><td>train/grad_norm</td><td>▇▇▇█▅█▆▆▂▂▂▁</td></tr><tr><td>train/learning_rate</td><td>▆██▇▆▅▄▃▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▆▅▄▃▃▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁</td></tr><tr><td>val_f1</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr><tr><td>val_precision</td><td>▁</td></tr><tr><td>val_recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.71875</td></tr><tr><td>eval/f1</td><td>0.65841</td></tr><tr><td>eval/loss</td><td>0.69987</td></tr><tr><td>eval/precision</td><td>0.62153</td></tr><tr><td>eval/recall</td><td>0.71875</td></tr><tr><td>eval/runtime</td><td>1.8796</td></tr><tr><td>eval/samples_per_second</td><td>17.025</td></tr><tr><td>eval/steps_per_second</td><td>1.064</td></tr><tr><td>test/accuracy</td><td>0.71875</td></tr><tr><td>test/f1</td><td>0.65841</td></tr><tr><td>test/loss</td><td>0.69987</td></tr><tr><td>test/precision</td><td>0.62153</td></tr><tr><td>test/recall</td><td>0.71875</td></tr><tr><td>test/runtime</td><td>2.0402</td></tr><tr><td>test/samples_per_second</td><td>15.685</td></tr><tr><td>test/steps_per_second</td><td>0.98</td></tr><tr><td>test_accuracy</td><td>0.71875</td></tr><tr><td>test_f1</td><td>0.65841</td></tr><tr><td>test_loss</td><td>0.69987</td></tr><tr><td>test_precision</td><td>0.62153</td></tr><tr><td>test_recall</td><td>0.71875</td></tr><tr><td>total_flos</td><td>1.4879262111694848e+17</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>120</td></tr><tr><td>train/grad_norm</td><td>0.66779</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.2903</td></tr><tr><td>train_loss</td><td>0.81161</td></tr><tr><td>train_runtime</td><td>271.2471</td></tr><tr><td>train_samples_per_second</td><td>7.078</td></tr><tr><td>train_steps_per_second</td><td>0.442</td></tr><tr><td>val_accuracy</td><td>0.83594</td></tr><tr><td>val_f1</td><td>0.83615</td></tr><tr><td>val_loss</td><td>0.58893</td></tr><tr><td>val_precision</td><td>0.84874</td></tr><tr><td>val_recall</td><td>0.83594</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">vit_AdamW_0.0002</strong> at: <a href='https://wandb.ai/dl_project_sp25/ViT-Optimizer-Sweep/runs/06jzobdu' target=\"_blank\">https://wandb.ai/dl_project_sp25/ViT-Optimizer-Sweep/runs/06jzobdu</a><br> View project at: <a href='https://wandb.ai/dl_project_sp25/ViT-Optimizer-Sweep' target=\"_blank\">https://wandb.ai/dl_project_sp25/ViT-Optimizer-Sweep</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250419_224419-06jzobdu/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiments completed!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting ViT experiments with different learning rate schedulers...\")\n",
        "    os.environ[\"WANDB_PROJECT\"] = \"ViT-LR-Schedulers\"\n",
        "\n",
        "    # Option 1: Run all experiments (time-consuming)\n",
        "    # results = run_all_experiments()\n",
        "    # results_df = visualize_results(results)\n",
        "\n",
        "    # Option 2: Run a single experiment for testing\n",
        "    # run_single_experiment(0)  # Try the baseline experiment first\n",
        "\n",
        "    #option 3: Optimizer sweep:\n",
        "    # Define sweep configuration\n",
        "    sweep_config = {\n",
        "        \"method\": \"grid\",  # we can use \"grid\", \"random\", or \"bayes\"\n",
        "        \"metric\": {\n",
        "            \"name\": \"val_accuracy\",  # Metric to optimize\n",
        "            \"goal\": \"maximize\"       # Goal: maximize or minimize\n",
        "        },\n",
        "        \"parameters\": {\n",
        "            \"optimizer_name\": {\n",
        "                \"values\": [\"schedule_free_adamw\",\"AdamW\", \"SGD\", \"RMSProp\", \"AdaGrad\"]  # Optimizers to test\n",
        "            },\n",
        "            \"learning_rate\": {\n",
        "                \"values\": [2e-5, 2e-4, 2e-3, 2e-2, 2e-1]  # Learning rates to test\n",
        "            },\n",
        "            \"batch_size\": {\n",
        "                \"values\": [16]  # Fixed batch size\n",
        "            },\n",
        "            \"num_epochs\": {\n",
        "                \"values\": [3]  # Fixed number of epochs\n",
        "            },\n",
        "            \"scheduler_name\": {\n",
        "                \"values\": [\"cosine\"]  # Fixed scheduler for simplicity\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Initialize the sweep\n",
        "    sweep_id = wandb.sweep(sweep_config, project=\"ViT-Optimizer-Sweep\")\n",
        "    \n",
        "    # Start the sweep agent\n",
        "    wandb.agent(sweep_id, function=run_optimizer_sweep)\n",
        "    \n",
        "\n",
        "    print(\"Experiments completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dON4fJvq0Ulv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                          optimizer_name  learning_rate  val_accuracy\n",
            "optimizer_name                                                       \n",
            "AdaGrad                          AdaGrad         0.0002      0.796875\n",
            "AdamW                              AdamW         0.0002      0.835938\n",
            "RMSProp                          RMSProp         0.0002      0.820312\n",
            "SGD                                  SGD         0.0200      0.812500\n",
            "schedule_free_adamw  schedule_free_adamw         0.0002      0.843750\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/3x/ww_4869j0gq7501tpsbdxh0m0000gq/T/ipykernel_96466/1863742853.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda group: group.loc[group[\"val_accuracy\"].idxmax()])\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the exported CSV file\n",
        "df = pd.read_csv(\"wandb_export.csv\")\n",
        "\n",
        "# Group by optimizer and find the best learning rate for each\n",
        "best_lr_per_optimizer = (\n",
        "    df.groupby(\"optimizer_name\")\n",
        "    .apply(lambda group: group.loc[group[\"val_accuracy\"].idxmax()])\n",
        "    [[\"optimizer_name\", \"learning_rate\", \"val_accuracy\"]]\n",
        ")\n",
        "\n",
        "print(best_lr_per_optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJqA_rlc0Ulv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
