{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqerD5hLLTXT"
      },
      "outputs": [],
      "source": [
        "# pip install transformers accelerate qwen-vl-utils==0.0.8 torch torchvision\n",
        "# pip install git+https://github.com/jacobgil/pytorch-grad-cam.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# !pip install unsloth\n",
        "# !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
        "\n",
        "# %%capture\n",
        "# Normally using pip install unsloth (the above code) is enough\n",
        "# Temporarily as of Jan 31st 2025, Colab has some issues with Pytorch\n",
        "# Using pip install unsloth will take 3 minutes, whilst the below takes <1 minute:\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
        "!pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth\n",
        "\n",
        "!pip uninstall -y transformers\n",
        "!pip install git+https://github.com/huggingface/transformers.git@1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf\n",
        "# there are some issues with using the qwen VL 3B model: https://github.com/huggingface/transformers/issues/36193\n",
        "# so i have to uninstall and reinstall the transformers above\n",
        "# might change to new model later on. update: i am using the official qwen model instead"
      ],
      "metadata": {
        "id": "5ODtswE6gccz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qwen-vl-utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rl6Am-FShxzX",
        "outputId": "c773f041-6cad-4842-8168-8acda16f6751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qwen-vl-utils\n",
            "  Downloading qwen_vl_utils-0.0.10-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting av (from qwen-vl-utils)\n",
            "  Downloading av-14.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from qwen-vl-utils) (24.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from qwen-vl-utils) (11.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from qwen-vl-utils) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->qwen-vl-utils) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->qwen-vl-utils) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->qwen-vl-utils) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->qwen-vl-utils) (2025.1.31)\n",
            "Downloading qwen_vl_utils-0.0.10-py3-none-any.whl (6.7 kB)\n",
            "Downloading av-14.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av, qwen-vl-utils\n",
            "Successfully installed av-14.2.0 qwen-vl-utils-0.0.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastVisionModel\n",
        "from transformers import AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
        "    load_in_4bit=True,\n",
        "    use_gradient_checkpointing=\"unsloth\"\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pVaOnxmLz74",
        "outputId": "05094180-5afa-4b32-dafe-63da1563a0d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.2.15: Fast Qwen2_Vl vision patching. Transformers: 4.49.0.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "EX2u0Nz2gbgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upload an image with \"im.jpeg\" name on google colab or change the image_path.\n",
        "image_path = \"im.jpeg\"\n",
        "question = \"What is shown in this image?\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"image\": image_path},\n",
        "            {\"type\": \"text\", \"text\": question},\n",
        "        ],\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "4rSPNoGsh-MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "image_inputs, video_inputs = process_vision_info(messages)\n",
        "inputs = processor(\n",
        "    text=[text],\n",
        "    images=image_inputs,\n",
        "    videos=video_inputs,\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to('cuda')"
      ],
      "metadata": {
        "id": "DFsb1SF7itaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_new_tokens = 10\n",
        "attention_maps = []\n",
        "\n",
        "for _ in range(max_new_tokens):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    next_token = outputs.logits[:, -1, :].argmax(dim=-1)\n",
        "    attention_maps.append(outputs.attentions)\n",
        "\n",
        "    # if next_token.item() == tokenizer.eos_token_id:\n",
        "    #     break\n",
        "\n",
        "    inputs['input_ids'] = torch.cat([inputs['input_ids'], next_token.unsqueeze(0)], dim=-1)\n",
        "    inputs['attention_mask'] = torch.cat([inputs['attention_mask'], torch.ones_like(next_token).unsqueeze(0)], dim=-1)\n",
        "\n",
        "generated_text = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n",
        "answer = generated_text.split(\"Answer: \")[-1].strip()"
      ],
      "metadata": {
        "id": "uuZXEJr9iu3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVkLaVAJlrFw",
        "outputId": "a4c53617-0b7e-4920-e9b4-3a7b15794683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "system\n",
            "You are a helpful assistant.\n",
            "user\n",
            "What is shown in this image?\n",
            "assistant\n",
            "The image shows a person standing outdoors in a scenic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKFxJJ6ejO9j",
        "outputId": "96cc2d20-10de-4676-9d01-260aee6e4190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1036, 152064])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Om0TLfiqlUhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def visualize_attention(attention_map, word, layer_idx):\n",
        "    # Assuming the last token's attention is what we want\n",
        "    att_map = attention_map[-1][0, -1].mean(dim=0).cpu().detach().numpy()\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(att_map, cmap='viridis')\n",
        "    plt.title(f\"Attention Map - Layer {layer_idx}, Word: {word}\")\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "\n",
        "answer_words = answer.split()\n",
        "for word_idx, word in enumerate(answer_words):\n",
        "    print(f\"Visualizing attention for word: {word}\")\n",
        "    for layer_idx, layer_attention in enumerate(attention_maps[word_idx]):\n",
        "        visualize_attention(layer_attention, word, layer_idx)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "XP-nJjM8lklT",
        "outputId": "f2ba82a3-fb37-4655-d28f-e75694dc94b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visualizing attention for word: system\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'NoneType' object is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-f1d2ceb4aec4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Visualizing attention for word: {word}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_attention\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mvisualize_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_attention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kbeYDTpGmVC3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}